{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UUSJPdr3Ge_"
      },
      "source": [
        "# Starter code and data\n",
        "\n",
        "First, perform the required imports for your code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CRwuwhoJ3Knl"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import sys\n",
        "import pylab\n",
        "import itertools\n",
        "\n",
        "TINY = 1e-30\n",
        "EPS = 1e-4\n",
        "nax = np.newaxis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNLvRXdy3NDO"
      },
      "source": [
        "If you're using colaboratory, this following script creates a folder - here we used 'CSC413/A1' - in order to download and store the data. If you're not using colaboratory, then set the path to wherever you want the contents to be stored at locally.\n",
        "\n",
        "You can also manually download and unzip the data from [http://www.cs.toronto.edu/~jba/a1_data.tar.gz] and put them in the same folder as where you store this notebook. \n",
        "\n",
        "Feel free to use a different way to access the files *data.pk* , *partially_trained.pk*, and *raw_sentences.txt*. \n",
        "\n",
        "The file *raw_sentences.txt* contains the sentences that we will be using for this assignment.\n",
        "These sentences are fairly simple ones and cover a vocabulary of only 250 words (+ 1 special `[MASK]` token word).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KUQjRpWqnkzk"
      },
      "outputs": [],
      "source": [
        "drive_location = 'data'\n",
        "PARTIALLY_TRAINED_MODEL = './data/partially_trained.pk'\n",
        "data_location = './data/data.pk'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qna9z_wJ3U5e"
      },
      "source": [
        "We have already extracted the 4-grams from this dataset and divided them into training, validation, and test sets.\n",
        "To inspect this data, run the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RD1LN16d3a0u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[MASK]\n",
            "all\n",
            "251\n",
            "['[MASK]', 'all', 'set', 'just', 'show', 'being', 'money', 'over', 'both', 'years', 'four', 'through', 'during', 'go', 'still', 'children', 'before', 'police', 'office', 'million', 'also', 'less', 'had', ',', 'including', 'should', 'to', 'only', 'going', 'under', 'has', 'might', 'do', 'them', 'good', 'around', 'get', 'very', 'big', 'dr.', 'game', 'every', 'know', 'they', 'not', 'world', 'now', 'him', 'school', 'several', 'like', 'did', 'university', 'companies', 'these', 'she', 'team', 'found', 'where', 'right', 'says', 'people', 'house', 'national', 'some', 'back', 'see', 'street', 'are', 'year', 'home', 'best', 'out', 'even', 'what', 'said', 'for', 'federal', 'since', 'its', 'may', 'state', 'does', 'john', 'between', 'new', ';', 'three', 'public', '?', 'be', 'we', 'after', 'business', 'never', 'use', 'here', 'york', 'members', 'percent', 'put', 'group', 'come', 'by', '$', 'on', 'about', 'last', 'her', 'of', 'could', 'days', 'against', 'times', 'women', 'place', 'think', 'first', 'among', 'own', 'family', 'into', 'each', 'one', 'down', 'because', 'long', 'another', 'such', 'old', 'next', 'your', 'market', 'second', 'city', 'little', 'from', 'would', 'few', 'west', 'there', 'political', 'two', 'been', '.', 'their', 'much', 'music', 'too', 'way', 'white', ':', 'was', 'war', 'today', 'more', 'ago', 'life', 'that', 'season', 'company', '-', 'but', 'part', 'court', 'former', 'general', 'with', 'than', 'those', 'he', 'me', 'high', 'made', 'this', 'work', 'up', 'us', 'until', 'will', 'ms.', 'while', 'officials', 'can', 'were', 'country', 'my', 'called', 'and', 'program', 'have', 'then', 'is', 'it', 'an', 'states', 'case', 'say', 'his', 'at', 'want', 'in', 'any', 'as', 'if', 'united', 'end', 'no', ')', 'make', 'government', 'when', 'american', 'same', 'how', 'mr.', 'other', 'take', 'which', 'department', '--', 'you', 'many', 'nt', 'day', 'week', 'play', 'used', \"'s\", 'though', 'our', 'who', 'yesterday', 'director', 'most', 'president', 'law', 'man', 'a', 'night', 'off', 'center', 'i', 'well', 'or', 'without', 'so', 'time', 'five', 'the', 'left']\n",
            "[[ 28  26  90 144]\n",
            " [184  44 249 117]\n",
            " [183  32  76 122]\n",
            " [117 247 201 186]\n",
            " [223 190 249   6]\n",
            " [ 42  74  26  32]\n",
            " [242  32 223  32]\n",
            " [223  32 158 144]\n",
            " [ 74  32 221  32]\n",
            " [ 42 192  91  68]]\n"
          ]
        }
      ],
      "source": [
        "data = pickle.load(open(data_location, 'rb'))\n",
        "print(data['vocab'][0]) # First word in vocab is [MASK] \n",
        "print(data['vocab'][1]) \n",
        "print(len(data['vocab'])) # Number of words in vocab\n",
        "print(data['vocab']) # All the words in vocab\n",
        "print(data['train_inputs'][:10]) # 10 example training instances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXd2Msqs3fPQ"
      },
      "source": [
        "Now `data` is a Python dict which contains the vocabulary, as well as the inputs and targets for all three splits of the data. `data['vocab']` is a list of the 251 words in the dictionary; `data['vocab'][0]` is the word with index 0, and so on. `data['train_inputs']` is a 372,500 x 4 matrix where each row gives the indices of the 4 consecutive context words for one of the 372,500 training cases.\n",
        "The validation and test sets are handled analogously.\n",
        "\n",
        "Even though you only have to modify two specific locations in the code, you may want to read through this code before starting the assignment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa9ggqxJPPs0"
      },
      "source": [
        "# Part 4: Linear Embedding -- GloVe (3pts)\n",
        "\n",
        "In this section we will be implementing a simplified version of [GloVe](https://nlp.stanford.edu/pubs/glove.pdf).\n",
        "Given a corpus with $V$ distinct words, we define the co-occurrence matrix $X\\in \\mathbb{N}^{V\\times V}$ with entries $X_{ij}$ representing the frequency of the $i$-th word and $j$-th word in the corpus appearing in the same *context* - in our case the adjacent words. The co-occurrence matrix can be *symmetric* (i.e., $X_{ij} = X_{ji}$) if the order of the words do not matter, or *asymmetric* (i.e., $X_{ij} \\neq X_{ji}$) if we wish to distinguish the counts for when $i$-th word appears before $j$-th word. \n",
        "GloVe aims to find a $d$-dimensional embedding of the words that preserves properties of the co-occurrence matrix by representing the $i$-th word with two $d$-dimensional vectors $\\mathbf{w}_i,\\tilde{\\mathbf{w}}_i \\in\\mathbb{R}^d$, as well as two scalar biases $b_i, \\tilde{b}_i\\in\\mathbb{R}$. Typically we have the dimension of the embedding $d$ much smaller than the number of words $V$. This objective can be written as:\n",
        "\n",
        "$$L(\\{\\mathbf{w}_i,\\tilde{\\mathbf{w}}_i,b_i, \\tilde{b}_i\\}_{i=1}^V) = \\sum_{i,j=1}^V (\\mathbf{w}_i^\\top\\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j - \\log X_{ij})^2$$\n",
        "\n",
        "Note that each word is represented by two $d$-dimensional embedding vectors $\\mathbf{w}_i, \\tilde{\\mathbf{w}}_i$ and two scalar biases $b_i, \\tilde{b}_i$.  When the bias terms are omitted and we tie the two embedding vectors $\\mathbf{w}_i =\\tilde{\\mathbf{w}}_i$, then GloVe corresponds to finding a rank-$d$ symmetric factorization of the co-occurrence matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKbDkmuGoTCC"
      },
      "source": [
        "## 4.2 Expression for the Vectorized Loss function [0.5pt] \\[Type 1\\]\n",
        " In practice, we concatenate the $V$ embedding vectors into matrices $\\mathbf{W}, \\tilde{\\mathbf{W}} \\in \\mathbb{R}^{V \\times d}$ and bias (column) vectors $\\mathbf{b}, \\tilde{\\mathbf{b}} \\in \\mathbb{R}^{V}$, where $V$ denotes the number of distinct words as described in the introduction. Rewrite the loss function $L$ (Eq. 1) in a vectorized format in terms of $\\mathbf{W}, \\tilde{\\mathbf{W}}, \\mathbf{b}, \\tilde{\\mathbf{b}}, X$. You are allowed to use elementwise operations such as addition and subtraction as well as matrix operations such as the Frobenius norm and/or trace operator in your answer.\n",
        "        \n",
        "*Hint: Use the all-ones column vector $\\mathbf{1} = [1 \\dots 1]^{T} \\in \\mathbb{R}^{V}$. You can assume the bias vectors are column vectors, i.e. implicitly a matrix with $V$ rows and 1 column: $\\mathbf{b}, \\tilde{\\mathbf{b}} \\in \\mathbb{R}^{V \\times 1}$*\n",
        "\n",
        "*Hint: To prompt a GPT-like model, try naively copy-pasting the question, generate several answers several times, and check the generated answers.*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**:\n",
        "\n",
        "The vectorized form of the loss function is given by:\n",
        "\n",
        "$$L(\\mathbf{W},\\tilde{\\mathbf{W}},\\mathbf{b},\\tilde{\\mathbf{b}}) = \\left|(\\mathbf{W}^\\top\\tilde{\\mathbf{W}} + \\mathbf{b} \\mathbf{1}^\\top + \\mathbf{1} \\tilde{\\mathbf{b}}^\\top - \\log X)\\right|_F^2$$\n",
        "\n",
        "where $\\mathbf{1} \\in \\mathbb{R}^{V}$ is an all-ones column vector, and $\\left|\\cdot\\right|_F$ is the Frobenius norm.\n",
        "\n",
        "ChatGPT trace:\n",
        "\n",
        "![](assets/7qZWpvKHInhFk_p5bc3PESZD96GSWGSP93-vrsHz8OQ.original.fullsize.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vQIRZynyGpl"
      },
      "source": [
        "## 4.3. Expression for gradient $\\frac{\\partial L}{\\partial \\mathbf{W}}$ \\[0.5pt\\] \\[Type 2\\]\n",
        "\n",
        "Write the vectorized expression for $\\frac{\\partial L}{\\partial \\mathbf{W}}$, the gradient of the loss function $L$ with respect to the embedding matrix $\\mathbf{W}$. The gradient should be a function of $\\mathbf{W}, \\tilde{\\mathbf{W}}, \\mathbf{b}, \\tilde{\\mathbf{b}}, X$. \n",
        "\n",
        "*Hint: Make sure that the shape of the gradient is equivalent to the shape of the matrix. You can use the all-ones vector as in the previous question.*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**:\n",
        "\n",
        "The gradient of the loss function $L$ with respect to the embedding matrix $\\mathbf{W}$ is given by:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial \\mathbf{W}} = 2(\\mathbf{W}^\\top\\tilde{\\mathbf{W}} + \\mathbf{b} \\mathbf{1}^\\top + \\mathbf{1} \\tilde{\\mathbf{b}}^\\top - \\log X)\\tilde{\\mathbf{W}}$$\n",
        "\n",
        "**ChatGPT trace**:\n",
        "\n",
        "![](./assets/0YqF31v7XCUEnYE8hc44KdS7LtHGVd9_kP2dJlxfUW4.original.fullsize.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQJrG7fkpEOe"
      },
      "source": [
        "## 4.4 Implement Vectorized Loss Function [1pt] \\[Type 2\\]\n",
        "\n",
        "Implement the `loss_GloVe()` function of GloVe. \n",
        "\n",
        "**See** `YOUR CODE HERE` **Comment below for where to complete the code**\n",
        "\n",
        "Note that you need to implement both the loss for an *asymmetric* model (from your answer in question 1.2) and the loss for a *symmetric* model which uses the same embedding matrix $\\mathbf{W}$ and bias vector $\\mathbf{b}$ for the first and second word in the co-occurrence, i.e. $\\tilde{\\mathbf{W}} = \\mathbf{W}$ and $\\tilde{\\mathbf{b}} = \\mathbf{b}$ in the original loss.\n",
        "        \n",
        "*Hint: You may take advantage of NumPy's broadcasting feature for the bias vectors: https://numpy.org/doc/stable/user/basics.broadcasting.html*\n",
        "\n",
        "We have provided a few functions for training the embedding:\n",
        "\n",
        "*   `calculate_log_co_occurence` computes the log co-occurrence matrix of a given corpus\n",
        "*   `train_GloVe` runs momentum gradient descent to optimize the embedding\n",
        "*   `loss_GloVe`: **TO BE IMPLEMENTED.**\n",
        "  * INPUT \n",
        "      * V x d matrix `W` (collection of $V$ embedding vectors, each $d$-dimensional)\n",
        "      * V x d matrix `W_tilde` \n",
        "      * V x 1 vector `b` (collection of $V$ bias terms)\n",
        "      * V x 1 vector `b_tilde`\n",
        "      * V x V log co-occurrence matrix. \n",
        "  * OUTPUT\n",
        "      * loss of the GloVe objective\n",
        "*   `grad_GloVe`: **TO BE IMPLEMENTED.**\n",
        "  * INPUT:\n",
        "      * V x d matrix `W` (collection of $V$ embedding vectors, each $d$-dimensional), embedding for first word; \n",
        "      * V x d matrix `W_tilde`, embedding for second word; \n",
        "      * V x 1 vector `b` (collection of $V$ bias terms); \n",
        "      * V x 1 vector `b_tilde`, bias for second word; \n",
        "      * V x V log co-occurrence matrix. \n",
        "  * OUTPUT:\n",
        "      * V x d matrix `grad_W` containing the gradient of the loss function w.r.t. `W`;\n",
        "      * V x d matrix `grad_W_tilde` containing the gradient of the loss function w.r.t. `W_tilde`;\n",
        "      * V x 1 vector `grad_b` which is the gradient of the loss function w.r.t. `b`. \n",
        "      * V x 1 vector `grad_b_tilde` which is the gradient of the loss function w.r.t. `b_tilde`. \n",
        "\n",
        "Run the code to compute the co-occurence matrix.\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rw0IToBap3E2"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(data['vocab']) # Number of vocabs\n",
        "\n",
        "def calculate_log_co_occurence(word_data, symmetric=False):\n",
        "  \"Compute the log-co-occurence matrix for our data.\"\n",
        "  log_co_occurence = np.zeros((vocab_size, vocab_size))\n",
        "  for input in word_data:\n",
        "    # Note: the co-occurence matrix may not be symmetric\n",
        "    log_co_occurence[input[0], input[1]] += 1\n",
        "    log_co_occurence[input[1], input[2]] += 1\n",
        "    log_co_occurence[input[2], input[3]] += 1\n",
        "    # Diagonal entries are just the frequency of the word\n",
        "    log_co_occurence[input[0], input[0]] += 1\n",
        "    log_co_occurence[input[1], input[1]] += 1\n",
        "    log_co_occurence[input[2], input[2]] += 1\n",
        "    log_co_occurence[input[3], input[3]] += 1\n",
        "    # If we want symmetric co-occurence can also increment for these.\n",
        "    if symmetric:\n",
        "      log_co_occurence[input[1], input[0]] += 1\n",
        "      log_co_occurence[input[2], input[1]] += 1\n",
        "      log_co_occurence[input[3], input[2]] += 1\n",
        "  delta_smoothing = 0.5  # A hyperparameter.  You can play with this if you want.\n",
        "  log_co_occurence += delta_smoothing  # Add delta so log doesn't break on 0's.\n",
        "  log_co_occurence = np.log(log_co_occurence)\n",
        "  return log_co_occurence\n",
        "\n",
        "asym_log_co_occurence_train = calculate_log_co_occurence(data['train_inputs'], symmetric=False)\n",
        "asym_log_co_occurence_valid = calculate_log_co_occurence(data['valid_inputs'], symmetric=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vbozDCFp8lD"
      },
      "source": [
        "* [ ] **TO BE IMPLEMENTED**: Implement the loss function. You should vectorize the computation, i.e. not loop over every word.\n",
        "\n",
        "*Hint: To prompt a GPT-like model, you may include in your prompt the vectorized loss formula derived in 4.2.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "U1zltFcrqFnq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1328584.8540725093\n",
            "1262403.2244261734\n"
          ]
        }
      ],
      "source": [
        "def loss_GloVe(W, W_tilde, b, b_tilde, log_co_occurence):\n",
        "    \"\"\" Compute the GloVe loss given the parameters of the model. When W_tilde \n",
        "    and b_tilde are not given, then the model is symmetric (i.e. W_tilde = W,\n",
        "    b_tilde = b). \n",
        "\n",
        "    Args:\n",
        "      W: word embedding matrix, dimension V x d where V is vocab size and d\n",
        "        is the embedding dimension\n",
        "      W_tilde: for asymmetric GloVe model, a second word embedding matrix, with\n",
        "        dimensions V x d\n",
        "      b: bias vector, dimension V x 1\n",
        "      b_tilde: for asymmetric GloVe model, a second bias vector, dimension V x 1\n",
        "      log_co_occurence: V x V log co-occurrence matrix (log X)\n",
        "\n",
        "    Returns:\n",
        "      loss: a scalar (float) for GloVe loss\n",
        "\n",
        "    $$\n",
        "    L(\\mathbf{W}, \\tilde{\\mathbf{W}}, \\mathbf{b}, \\tilde{\\mathbf{b}})=\\left|\\left(\\mathbf{W}^{\\top} \\tilde{\\mathbf{W}}+\\mathbf{b} \\mathbf{1}^{\\top}+\\mathbf{1} \\tilde{\\mathbf{b}}^{\\top}-\\log X\\right)\\right|_F^2\n",
        "    $$\n",
        "    \"\"\"\n",
        "    n, _ = log_co_occurence.shape\n",
        "    # Symmetric Case, no W_tilde and b_tilde\n",
        "    if W_tilde is None and b_tilde is None:\n",
        "        # Symmetric model\n",
        "        ###########################   YOUR CODE HERE  ##############################\n",
        "        loss = np.sum((np.matmul(W, W.T) + b + b.T - log_co_occurence)**2)\n",
        "        ############################################################################\n",
        "    else:\n",
        "        # Asymmetric model\n",
        "        ###########################   YOUR CODE HERE  ##############################\n",
        "        loss = np.sum((np.matmul(W, W_tilde.T) + b + b_tilde.T - log_co_occurence)**2)\n",
        "        ############################################################################\n",
        "    return loss\n",
        "\n",
        "\n",
        "# testing the loss function\n",
        "W = np.random.randn(vocab_size, 16)\n",
        "W_tilde = np.random.randn(vocab_size, 16)\n",
        "b = np.random.randn(vocab_size, 1)\n",
        "b_tilde = np.random.randn(vocab_size, 1)\n",
        "\n",
        "print(loss_GloVe(W, W_tilde, b, b_tilde, asym_log_co_occurence_train))\n",
        "print(loss_GloVe(W, None, b, None, asym_log_co_occurence_train))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ChatGPT trace:**\n",
        "\n",
        "![](assets/BTJ149_fKYVedcP0hMF8_Wc2keHhjYcD-d8G6DfVu5Y.original.fullsize.png)\n",
        "\n",
        "![](assets/TR3kGYrAqYVzoGnrus-54SdmSgarekZdQ0CZV0KUjPY.original.fullsize.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzWek3lP0p2e"
      },
      "source": [
        "## 4.5.  Implement the gradient update of GloVe. \\[1pt\\] \\[Type 2\\]\n",
        "\n",
        "Implement the `grad_GloVe()` function which computes the gradient of GloVe.\n",
        "\n",
        "**See** `YOUR CODE HERE` **Comment below for where to complete the code**\n",
        "\n",
        "\n",
        "Again, note that you need to implement the gradient for both the symmetric and asymmetric models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNnKkMy-d2bB"
      },
      "source": [
        "* [ ] **TO BE IMPLEMENTED**: Calculate the gradient of the loss function w.r.t. the parameters $W$, $\\tilde{W}$, $\\mathbf{b}$, and $\\mathbf{b}$. You should vectorize the computation, i.e. not loop over every word.\n",
        "\n",
        "*Hint: To prompt a GPT-like model, you may include the GloVe loss implementation in 4.4.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "LbpkXeaAdwnj"
      },
      "outputs": [],
      "source": [
        "def grad_GloVe(W, W_tilde, b, b_tilde, log_co_occurence):\n",
        "  \"\"\"Return the gradient of GloVe objective w.r.t its parameters\n",
        "  Args:\n",
        "    W: word embedding matrix, dimension V x d where V is vocab size and d\n",
        "      is the embedding dimension\n",
        "    W_tilde: for asymmetric GloVe model, a second word embedding matrix, with\n",
        "      dimensions V x d\n",
        "    b: bias vector, dimension V x 1\n",
        "    b_tilde: for asymmetric GloVe model, a second bias vector, dimension V x 1\n",
        "    log_co_occurence: V x V log co-occurrence matrix (log X)\n",
        "  \n",
        "  Returns:\n",
        "    grad_W: gradient of the loss wrt W, dimension V x d\n",
        "    grad_W_tilde: gradient of the loss wrt W_tilde, dimension V x d. Return \n",
        "      None if W_tilde is None.\n",
        "    grad_b: gradient of the loss wrt b, dimension V x 1\n",
        "    grad_b_tilde: gradient of the loss wrt b, dimension V x 1. Return \n",
        "      None if b_tilde is None.\n",
        "  $$\n",
        "  \\begin{aligned}\n",
        "  \\frac{\\partial L}{\\partial \\mathbf{W}} & =2\\left(\\mathbf{W}^{\\top} \\tilde{\\mathbf{W}}+\\mathbf{b} \\mathbf{1}^{\\top}+\\mathbf{1} \\tilde{\\mathbf{b}}^{\\top}-\\log X\\right) \\tilde{\\mathbf{W}}^{\\top} \\\\\n",
        "  \\frac{\\partial L}{\\partial \\tilde{\\mathbf{W}}} & =2\\left(\\mathbf{W}^{\\top} \\tilde{\\mathbf{W}}+\\mathbf{b} \\mathbf{1}^{\\top}+\\mathbf{1} \\tilde{\\mathbf{b}}^{\\top}-\\log X\\right) \\mathbf{W}^{\\top} \\\\\n",
        "  \\frac{\\partial L}{\\partial \\mathbf{b}} & =2\\left(\\mathbf{W}^{\\top} \\tilde{\\mathbf{W}}+\\mathbf{b} \\mathbf{1}^{\\top}+\\mathbf{1} \\tilde{\\mathbf{b}}^{\\top}-\\log X\\right) \\mathbf{1} \\\\\n",
        "  \\frac{\\partial L}{\\partial \\tilde{\\mathbf{b}}} & =2\\left(\\mathbf{W}^{\\top} \\tilde{\\mathbf{W}}+\\mathbf{b} \\mathbf{1}^{\\top}+\\mathbf{1} \\tilde{\\mathbf{b}}^{\\top}-\\log X\\right) \\mathbf{1}\n",
        "  \\end{aligned}\n",
        "  $$\n",
        "  \"\"\"\n",
        "  n,_ = log_co_occurence.shape\n",
        "  \n",
        "  # print(W.shape)\n",
        "  # print(W_tilde.shape)\n",
        "  # print(b.shape)\n",
        "  # print(b_tilde.shape)\n",
        "  # print(log_co_occurence.shape)\n",
        "  \n",
        "  # (251, 16)\n",
        "  # (251, 16)\n",
        "  # (251, 1)\n",
        "  # (251, 1)\n",
        "  # (251, 251)\n",
        "  \n",
        "  if W_tilde is None and b_tilde is None:\n",
        "    # Symmmetric case\n",
        "    error = np.matmul(W,W.T) + b + b.T - log_co_occurence\n",
        "    grad_W = 2 * np.matmul(error, W)\n",
        "    grad_W_tilde = None\n",
        "    grad_b = 2 * np.matmul(error, np.ones((n,1)))\n",
        "    grad_b_tilde = None\n",
        "  else:\n",
        "    # Asymmetric case\n",
        "    error = np.matmul(W,W_tilde.T) + b + b_tilde.T - log_co_occurence\n",
        "    grad_W = 2 * np.matmul(error, W_tilde)\n",
        "    grad_W_tilde = 2 * np.matmul(error.T, W)\n",
        "    grad_b = 2 * np.matmul(error, np.ones((n,1)))\n",
        "    grad_b_tilde = 2 * np.matmul(error, np.ones((n,1)))\n",
        "    \n",
        "  return grad_W, grad_W_tilde, grad_b, grad_b_tilde\n",
        "\n",
        "# np.random.seed(0)\n",
        "\n",
        "# # Store the final losses for graphing\n",
        "# init_variance = 0.05  # A hyperparameter.  You can play with this if you want.\n",
        "# embedding_dim = 16\n",
        "# W = init_variance * np.random.normal(size=(vocab_size, embedding_dim))\n",
        "# W_tilde = init_variance * np.random.normal(size=(vocab_size, embedding_dim))\n",
        "# b = init_variance * np.random.normal(size=(vocab_size, 1))\n",
        "# b_tilde = init_variance * np.random.normal(size=(vocab_size, 1))\n",
        "\n",
        "# grad_GloVe(W, W_tilde, b, b_tilde, asym_log_co_occurence_train)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ChatGPT trace**\n",
        "\n",
        "![](assets/Gyt5K9STj1baO5i2xu6aUq5Y6_L-rl1amrcrmUa-140.original.fullsize.png)\n",
        "\n",
        "![](assets/IB16KZxcx7OI4xCJMHKL0GyUN2vKFFvBf3bCkYLlOx8.original.fullsize.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U5_CAJ0VxzI3"
      },
      "source": [
        "To help you debug your GloVe gradient computation, we have included a finite-difference gradien checker function defined below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "HNiv-P0or6gt"
      },
      "outputs": [],
      "source": [
        "def relative_error(a, b):\n",
        "    return np.abs(a - b) / (np.abs(a) + np.abs(b))\n",
        " \n",
        "def check_GloVe_gradients(W, W_tilde, b, b_tilde, log_co_occurence):\n",
        "    \"\"\"Check the computed gradients using finite differences.\"\"\"\n",
        "    np.random.seed(0)\n",
        "    np.seterr(all='ignore')  # suppress a warning which is harmless\n",
        "\n",
        "    # Obtain the analytical gradient\n",
        "    grad_W, grad_W_tilde, grad_b, grad_b_tilde = grad_GloVe(W, W_tilde, b, b_tilde, log_co_occurence)\n",
        "    grads_dict = {\"W\":grad_W, \"W_tilde\": grad_W_tilde, \n",
        "                      \"b\": grad_b, \"b_tilde\": grad_b_tilde}\n",
        "\n",
        "    params_dict = {\"W\":W, \"W_tilde\":W_tilde, \"b\":b, \"b_tilde\":b_tilde}\n",
        "\n",
        "    # Check that the shapes of the parameters and gradients match\n",
        "    for name in params_dict:\n",
        "      if params_dict[name] is None:\n",
        "        continue\n",
        "      dims = params_dict[name].shape\n",
        "      is_matrix = (len(dims) == 2)\n",
        "      if not is_matrix:\n",
        "        print()\n",
        "\n",
        "      if params_dict[name].shape != grads_dict[name].shape:\n",
        "        print('The gradient for {} should be size {} but is actually {}.'.format(\n",
        "            name, params_dict[name].shape, grads_dict[name].shape))\n",
        "        return\n",
        "\n",
        "      # Run finite difference for that param\n",
        "      for count in range(1000):\n",
        "        if is_matrix:\n",
        "            slc = np.random.randint(0, dims[0]), np.random.randint(0, dims[1])\n",
        "        else:\n",
        "            slc = np.random.randint(dims[0])\n",
        "        \n",
        "        params_dict_plus = params_dict.copy()\n",
        "        params_dict_plus[name] = params_dict[name].copy()\n",
        "        params_dict_plus[name][slc] += EPS\n",
        "        obj_plus = loss_GloVe(params_dict_plus[\"W\"], \n",
        "                              params_dict_plus[\"W_tilde\"], \n",
        "                              params_dict_plus[\"b\"],\n",
        "                              params_dict_plus[\"b_tilde\"],\n",
        "                              log_co_occurence)\n",
        "\n",
        "        params_dict_minus = params_dict.copy()\n",
        "        params_dict_minus[name] = params_dict[name].copy()\n",
        "        params_dict_minus[name][slc] -= EPS\n",
        "        obj_minus = loss_GloVe(params_dict_minus[\"W\"], \n",
        "                              params_dict_minus[\"W_tilde\"], \n",
        "                              params_dict_minus[\"b\"],\n",
        "                              params_dict_minus[\"b_tilde\"],\n",
        "                              log_co_occurence)\n",
        "\n",
        "        empirical = (obj_plus - obj_minus) / (2. * EPS)\n",
        "        exact = grads_dict[name][slc]\n",
        "        rel = relative_error(empirical, exact)\n",
        "        if rel > 5e-4:\n",
        "          print('The loss derivative has a relative error of {}, which is too large for param {}.'.format(rel, name))\n",
        "          print(empirical)\n",
        "          print(exact)\n",
        "          return False\n",
        "      print('The gradient for {} looks OK.'.format(name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TyVFJtax8Dd"
      },
      "source": [
        "Run the cell below to check if your `grad_GloVe` function passes the checker. The function will check for both the symmetric and asymmetric loss, for each of the parameter variables whether its gradient computation looks ok. The expected output is:\n",
        "\n",
        "```\n",
        "Checking asymmetric loss gradient...\n",
        "The gradient for W looks OK.\n",
        "The gradient for W_tilde looks OK.\n",
        "The gradient for b looks OK.\n",
        "The gradient for b_tilde looks OK.\n",
        "\n",
        "Checking symmetric loss gradient...\n",
        "The gradient for W looks OK.\n",
        "The gradient for b looks OK.\n",
        "```\n",
        "\n",
        "Note: If you update the `grad_GloVe` cell while debugging, make sure to run the `grad_GloVe` cell again before re-running the cell below to check the gradient.\n",
        "\n",
        "- [ ] **TODO**: Run this cell below to check the gradient implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "SY5scli4sPKP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking asymmetric loss gradient...\n",
            "The gradient for W looks OK.\n",
            "The gradient for W_tilde looks OK.\n",
            "The gradient for b looks OK.\n",
            "The loss derivative has a relative error of 1.0, which is too large for param b_tilde.\n",
            "-18.510645750211552\n",
            "87.45658387528688\n",
            "\n",
            "Checking symmetric loss gradient...\n",
            "The loss derivative has a relative error of 1.0, which is too large for param W.\n",
            "-0.4959737998433411\n",
            "0.47961455453046875\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "# Store the final losses for graphing\n",
        "init_variance = 0.05  # A hyperparameter.  You can play with this if you want.\n",
        "embedding_dim = 16\n",
        "W = init_variance * np.random.normal(size=(vocab_size, embedding_dim))\n",
        "W_tilde = init_variance * np.random.normal(size=(vocab_size, embedding_dim))\n",
        "b = init_variance * np.random.normal(size=(vocab_size, 1))\n",
        "b_tilde = init_variance * np.random.normal(size=(vocab_size, 1))\n",
        "\n",
        "print(\"Checking asymmetric loss gradient...\")\n",
        "check_GloVe_gradients(W, W_tilde, b, b_tilde, asym_log_co_occurence_train)\n",
        "\n",
        "print(\"\\nChecking symmetric loss gradient...\")\n",
        "check_GloVe_gradients(W, None, b, None, asym_log_co_occurence_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXJBYGqX6hP_"
      },
      "source": [
        "Now that you have checked taht the gradient is correct, we define the training function for the model given the initial weights and ground truth log co-occurence matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sefu3T7u6jBL"
      },
      "outputs": [],
      "source": [
        "def train_GloVe(W, W_tilde, b, b_tilde, log_co_occurence_train, log_co_occurence_valid, n_epochs, do_print=False):\n",
        "  \"Traing W and b according to GloVe objective.\"\n",
        "  n,_ = log_co_occurence_train.shape\n",
        "  learning_rate = 0.05 / n  # A hyperparameter.  You can play with this if you want.\n",
        "  train_loss_list = np.zeros(n_epochs)\n",
        "  valid_loss_list = np.zeros(n_epochs)\n",
        "  vocab_size = log_co_occurence_train.shape[0]\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    grad_W, grad_W_tilde, grad_b, grad_b_tilde = grad_GloVe(W, W_tilde, b, b_tilde, log_co_occurence_train)\n",
        "    W = W - learning_rate * grad_W\n",
        "    b = b - learning_rate * grad_b\n",
        "    if not grad_W_tilde is None and not grad_b_tilde is None:\n",
        "      W_tilde = W_tilde - learning_rate * grad_W_tilde\n",
        "      b_tilde = b_tilde - learning_rate * grad_b_tilde\n",
        "    train_loss, valid_loss = loss_GloVe(W, W_tilde, b, b_tilde, log_co_occurence_train), loss_GloVe(W, W_tilde, b, b_tilde, log_co_occurence_valid)\n",
        "    if do_print:\n",
        "      print(f\"Average Train Loss: {train_loss / vocab_size}, Average valid loss: {valid_loss / vocab_size}, grad_norm: {np.sum(grad_W**2)}\")\n",
        "    train_loss_list[epoch] = train_loss / vocab_size\n",
        "    valid_loss_list[epoch] = valid_loss / vocab_size\n",
        "\n",
        "  return W, W_tilde, b, b_tilde, train_loss_list, valid_loss_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwmBLMvKGE9-"
      },
      "source": [
        "- [ ] **TODO**: Run this cell below to run an experiment training GloVe model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "eIbyEcyhFwDC"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### TODO: Run this cell ###\n",
        "np.random.seed(1)\n",
        "n_epochs = 500  # A hyperparameter.  You can play with this if you want.\n",
        "\n",
        "# Store the final losses for graphing\n",
        "do_print = False  # If you want to see diagnostic information during training\n",
        "init_variance = 0.1  # A hyperparameter.  You can play with this if you want.\n",
        "embedding_dim = 16\n",
        "W = init_variance * np.random.normal(size=(vocab_size, embedding_dim))\n",
        "W_tilde = init_variance * np.random.normal(size=(vocab_size, embedding_dim))\n",
        "b = init_variance * np.random.normal(size=(vocab_size, 1))\n",
        "b_tilde = init_variance * np.random.normal(size=(vocab_size, 1))\n",
        "\n",
        "# Run the training for the asymmetric and symmetric GloVe model\n",
        "Asym_W_final, Asym_W_tilde_final, Asym_b_final, Asym_b_tilde_final, Asym_train_loss_list, Asym_valid_loss_list = train_GloVe(W, W_tilde, b, b_tilde, asym_log_co_occurence_train, asym_log_co_occurence_valid, n_epochs, do_print=do_print)\n",
        "Sym_W_final, Sym_W_tilde_final, Sym_b_final, Sym_b_tilde_final, Sym_train_loss_list, Sym_valid_loss_list = train_GloVe(W, None, b, None, asym_log_co_occurence_train, asym_log_co_occurence_valid, n_epochs, do_print=do_print)\n",
        "\n",
        "# Plot the resulting training curve\n",
        "pylab.plot(Asym_train_loss_list, label=\"Asym Train Loss\", color='red')\n",
        "pylab.plot(Asym_valid_loss_list, label=\"Asym Valid Loss\", color='red', linestyle='--')\n",
        "pylab.plot(Sym_train_loss_list, label=\"Sym Train Loss\", color='blue')\n",
        "pylab.plot(Sym_valid_loss_list, label=\"Sym Valid Loss\", color='blue', linestyle='--')\n",
        "pylab.xlabel(\"Iterations\")\n",
        "pylab.ylabel(\"Average GloVe Loss\")\n",
        "pylab.title(\"Asymmetric and Symmetric GloVe Model on Asymmetric Log Co-Occurrence (Emb Dim={})\".format(embedding_dim))\n",
        "pylab.legend()\n",
        "pylab.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YwDZcOhjywe"
      },
      "source": [
        "# Part 5: Neural Language Model Network architecture (2pt)\n",
        "See the handout for the written questions in this part.\n",
        "\n",
        "## Answer the following questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBQmEPvazDkA"
      },
      "source": [
        "## 5.1. Number of parameters in neural network model \\[1pt\\] \\[Type 2\\]\n",
        "\n",
        "The trainable parameters of the model consist of 3 weight matrices and 2 sets of biases. What is the total number of trainable parameters in the model, as a function of $V,N,D,H$? \n",
        "\n",
        "In the diagram given, which part of the model (i.e., `word_embbeding_weights`, `embed_to_hid_weights`, `hid_to_output_weights`, `hid_bias`, or `output_bias`) has the largest number of trainable parameters if we have the constraint that $V \\gg H > D > N$? Note: The symbol $\\gg$ means ``much greater than\" Explain your reasoning. \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CJKJ7qi9zVAm"
      },
      "source": [
        "**Answer**\n",
        "\n",
        "The total number of trainable parameters in the model is given by the sum of the number of parameters in each of the 3 weight matrices and 2 sets of biases:\n",
        "\n",
        "    Word Embedding Weights matrix: V x D parameters\n",
        "    Embedding to Hidden Weights matrix: H x (N x D) parameters\n",
        "    Hidden to Output Weights matrix: V x H x N parameters\n",
        "    Hidden biases: H parameters\n",
        "    Output biases: V parameters\n",
        "\n",
        "Therefore, the total number of parameters is V x D + H x (N x D) + V x H x N + H + V.\n",
        "\n",
        "If we have the constraint that V ≫ H > D > N, then the part of the model with the largest number of parameters is the Hidden to Output Weights matrix, which has V x H x N parameters.\n",
        "\n",
        "**ChatGPT Trace**\n",
        "\n",
        "![](assets/uley6z0Jv3v3s61FrbVj0mQhd7YNAHb_4r-N5lzA6hk.original.fullsize.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx_h25zYzVWJ"
      },
      "source": [
        "## 5.2 Number of parameters in $n$-gram model \\[1pt\\] \\[Type 1\\]\n",
        "Another method for predicting the next words is an *n-gram model*, which was mentioned in Lecture 3. If we wanted to use an n-gram model with the same context length $N-1$ as our network (since we mask 1 of the $N$ words in our input), we'd need to store the counts of all possible $N$-grams. If we stored all the counts explicitly and suppose that we have $V$ words in the dictionary, how many entries would this table have?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "n4hTwsjTzXvi"
      },
      "source": [
        "**Answer**\n",
        "\n",
        "In an n-gram model with context length $N-1$, we need to store the counts of all possible $N$-grams. The number of possible $N$-grams is given by $V^{N}$. This is because for each of the $N$ words in the $N$-gram, there are $V$ possible words that could be chosen from the dictionary.\n",
        "\n",
        "Therefore, the table for storing the counts of all possible $N$-grams would have $V^{N}$ entries.\n",
        "\n",
        "**ChatGPT Trace**\n",
        "\n",
        "![](assets/zfVg9vhYcluKshljWFkFsUIrdQYzYQ0f9NN7j_qClhU.original.fullsize.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8FZTyrzlCNl"
      },
      "source": [
        "# Part 6: Training the Neural Network (2pts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua0qkOH1tqHw"
      },
      "source": [
        "In this part, you will learn to implement and train the neural language model from Figure 1. As described in the previous section, during training, we randomly sample one of the $N$ context words to replace with a `[MASK]` token. The goal is for the network to predict the word that was masked, at the corresponding output word position. In practice, this `[MASK]` token is assigned the index 0 in our dictionary. The weights $W^{(2)}$ = `hid_to_output_weights` now has the shape $NV \\times H$, as the output layer has $NV$ neurons, where the first $V$ output units are for predicting the first word, then the next $V$ are for predicting the second word, and so on. \n",
        "        We call this as *concatenating* output units across all word positions, i.e. the $(v + nV)$-th column is for the word $v$ in vocabulary for the $n$-th output word position. \n",
        "        Note here that the softmax is applied in chunks of $V$ as well, to give a valid probability distribution over the $V$ words (For simplicity we also include the `[MASK]` token as one of the possible prediction even though we know the target should not be this token). Only the output word positions that were masked in the input are included in the cross entropy loss calculation:\n",
        "\n",
        "$$C = -\\sum_{i}^{B}\\sum_{n}^{N}\\sum_{v}^{V} m^{(i)}_{n} (t^{(i)}_{v + nV} \\log y^{(i)}_{v + nV})$$\n",
        "\n",
        "Where:\n",
        "*  $y^{(i)}_{v + nV}$ denotes the output probability prediction from the neural network for the $i$-th training example for the word $v$ in the $n$-th output word. Denoting $z$ as the logits output, we define the output probability $y$ as a softmax on $z$ over contiguous chunks of $V$ units (see also Figure 1): \n",
        "\n",
        "$$y^{(i)}_{v + nV} = \\frac{e^{z^{(i)}_{v+nV}}}{\\sum_{l}^{V} e^{z^{(i)}_{l+nV}}}$$\n",
        "* $t^{(i)}_{v + nV}  \\in \\{0,1\\}$ is 1 if for the $i$-th training example, the word $v$ is the $n$-th word in context\n",
        "* $m^{(i)}_{n} \\in \\{0,1\\}$ is a mask that is set to 1 if we are predicting the $n$-th word position for the $i$-th example (because we had masked that word in the input), and 0 otherwise\n",
        "\n",
        "There are three classes defined in this part: `Params`, `Activations`, `Model`.\n",
        "You will make changes to `Model`, but it may help to read through `Params` and `Activations` first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "EfGEjB3QLNXf"
      },
      "outputs": [],
      "source": [
        "class Params(object):\n",
        "    \"\"\"A class representing the trainable parameters of the model. This class has five fields:\n",
        "    \n",
        "           word_embedding_weights, a matrix of size V x D, where V is the number of words in the vocabulary\n",
        "                   and D is the embedding dimension.\n",
        "           embed_to_hid_weights, a matrix of size H x ND, where H is the number of hidden units. The first D\n",
        "                   columns represent connections from the embedding of the first context word, the next D columns\n",
        "                   for the second context word, and so on. There are N context words.\n",
        "           hid_bias, a vector of length H\n",
        "           hid_to_output_weights, a matrix of size NV x H\n",
        "           output_bias, a vector of length NV\"\"\"\n",
        "\n",
        "    def __init__(self, word_embedding_weights, embed_to_hid_weights, hid_to_output_weights,\n",
        "                 hid_bias, output_bias):\n",
        "        self.word_embedding_weights = word_embedding_weights\n",
        "        self.embed_to_hid_weights = embed_to_hid_weights\n",
        "        self.hid_to_output_weights = hid_to_output_weights\n",
        "        self.hid_bias = hid_bias\n",
        "        self.output_bias = output_bias\n",
        "\n",
        "    def copy(self):\n",
        "        return self.__class__(self.word_embedding_weights.copy(), self.embed_to_hid_weights.copy(),\n",
        "                              self.hid_to_output_weights.copy(), self.hid_bias.copy(), self.output_bias.copy())\n",
        "\n",
        "    @classmethod\n",
        "    def zeros(cls, vocab_size, context_len, embedding_dim, num_hid):\n",
        "        \"\"\"A constructor which initializes all weights and biases to 0.\"\"\"\n",
        "        word_embedding_weights = np.zeros((vocab_size, embedding_dim))\n",
        "        embed_to_hid_weights = np.zeros((num_hid, context_len * embedding_dim))\n",
        "        hid_to_output_weights = np.zeros((vocab_size * context_len, num_hid))\n",
        "        hid_bias = np.zeros(num_hid)\n",
        "        output_bias = np.zeros(vocab_size * context_len)\n",
        "        return cls(word_embedding_weights, embed_to_hid_weights, hid_to_output_weights,\n",
        "                   hid_bias, output_bias)\n",
        "\n",
        "    @classmethod\n",
        "    def random_init(cls, init_wt, vocab_size, context_len, embedding_dim, num_hid):\n",
        "        \"\"\"A constructor which initializes weights to small random values and biases to 0.\"\"\"\n",
        "        word_embedding_weights = np.random.normal(0., init_wt, size=(vocab_size, embedding_dim))\n",
        "        embed_to_hid_weights = np.random.normal(0., init_wt, size=(num_hid, context_len * embedding_dim))\n",
        "        hid_to_output_weights = np.random.normal(0., init_wt, size=(vocab_size * context_len, num_hid))\n",
        "        hid_bias = np.zeros(num_hid)\n",
        "        output_bias = np.zeros(vocab_size * context_len)\n",
        "        return cls(word_embedding_weights, embed_to_hid_weights, hid_to_output_weights,\n",
        "                   hid_bias, output_bias)\n",
        "\n",
        "    ###### The functions below are Python's somewhat oddball way of overloading operators, so that\n",
        "    ###### we can do arithmetic on Params instances. You don't need to understand this to do the assignment.\n",
        "\n",
        "    def __mul__(self, a):\n",
        "        return self.__class__(a * self.word_embedding_weights,\n",
        "                              a * self.embed_to_hid_weights,\n",
        "                              a * self.hid_to_output_weights,\n",
        "                              a * self.hid_bias,\n",
        "                              a * self.output_bias)\n",
        "\n",
        "    def __rmul__(self, a):\n",
        "        return self * a\n",
        "\n",
        "    def __add__(self, other):\n",
        "        return self.__class__(self.word_embedding_weights + other.word_embedding_weights,\n",
        "                              self.embed_to_hid_weights + other.embed_to_hid_weights,\n",
        "                              self.hid_to_output_weights + other.hid_to_output_weights,\n",
        "                              self.hid_bias + other.hid_bias,\n",
        "                              self.output_bias + other.output_bias)\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        return self + -1. * other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "k6XFQUPsLSi7"
      },
      "outputs": [],
      "source": [
        "class Activations(object):\n",
        "    \"\"\"A class representing the activations of the units in the network. This class has three fields:\n",
        "\n",
        "        embedding_layer, a matrix of B x ND matrix (where B is the batch size, D is the embedding dimension,\n",
        "                and N is the number of input context words), representing the activations for the embedding \n",
        "                layer on all the cases in a batch. The first D columns represent the embeddings for the \n",
        "                first context word, and so on.\n",
        "        hidden_layer, a B x H matrix representing the hidden layer activations for a batch\n",
        "        output_layer, a B x V matrix representing the output layer activations for a batch\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_layer, hidden_layer, output_layer):\n",
        "        self.embedding_layer = embedding_layer\n",
        "        self.hidden_layer = hidden_layer\n",
        "        self.output_layer = output_layer\n",
        "\n",
        "def get_batches(inputs, batch_size, shuffle=True):\n",
        "    \"\"\"Divide a dataset (usually the training set) into mini-batches of a given size. This is a\n",
        "    'generator', i.e. something you can use in a for loop. You don't need to understand how it\n",
        "    works to do the assignment.\"\"\"\n",
        "\n",
        "    if inputs.shape[0] % batch_size != 0:\n",
        "        raise RuntimeError('The number of data points must be a multiple of the batch size.')\n",
        "    num_batches = inputs.shape[0] // batch_size\n",
        "\n",
        "    if shuffle:\n",
        "        idxs = np.random.permutation(inputs.shape[0])\n",
        "        inputs = inputs[idxs, :]\n",
        "\n",
        "    for m in range(num_batches):\n",
        "        yield inputs[m * batch_size:(m + 1) * batch_size, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuAXaDNll0lf"
      },
      "source": [
        "In this part of the assignment, you implement a method which computes the gradient using backpropagation.\n",
        "To start you out, the *Model* class contains several important methods used in training:\n",
        "\n",
        "\n",
        "*   `compute_activations` computes the activations of all units on a given input batch\n",
        "*   `compute_loss_derivative` computes the gradient with respect to the output logits $\\frac{\\partial C}{\\partial z}$\n",
        "*   `evaluate` computes the average cross-entropy loss for a given set of inputs and targets\n",
        "\n",
        "You will need to complete the implementation of two additional methods to complete the training, and print the outputs of the gradients. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVF5TxDgtqHx"
      },
      "source": [
        "## 6.1 Implement gradient with respect to output layer inputs [0.5pt] \\[Type 2\\]\n",
        "Implement a vectorized `compute_loss` function, which computes the total cross-entropy loss on a mini-batch according to Eq. 2. Look for the `## YOUR CODE HERE ##` comment for where to complete the code. The docstring provides a description of the inputs to the function. \n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**\n",
        "\n",
        "```python\n",
        "loss = -((expanded_target_batch * np.log(output_activations)).reshape(-1, self.context_len, len(self.vocab)) * target_mask).sum()\n",
        "```\n",
        "\n",
        "**ChatGPT Trace**\n",
        "\n",
        "![](assets/DnHHxgr6VQ3RejCWdBQHEquxdaJQf8BoN9rvlnYcRWc.original.fullsize.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyAoLfRPYZYx"
      },
      "source": [
        "## 6.2 Implement gradient with respect to parameters [1pt] \\[Type 2\\]\n",
        "`back_propagate` is the function which computes the gradient of the loss with respect to model parameters using backpropagation.\n",
        "It uses the derivatives computed by *compute_loss_derivative*.\n",
        "Some parts are already filled in for you, but you need to compute the matrices of derivatives for `embed_to_hid_weights`, `hid_bias`, `hid_to_output_weights`, and `output_bias`.\n",
        "These matrices have the same sizes as the parameter matrices (see previous section). These matrices have the same sizes as the parameter matrices. Look for the `## YOUR CODE HERE ##` comment for where to complete the code.\n",
        "\n",
        "In order to implement backpropagation efficiently, you need to express the computations in terms of matrix operations, rather than *for* loops.\n",
        "You should first work through the derivatives on pencil and paper.\n",
        "First, apply the chain rule to compute the derivatives with respect to individual units, weights, and biases.\n",
        "Next, take the formulas you've derived, and express them in matrix form.\n",
        "You should be able to express all of the required computations using only matrix multiplication, matrix transpose, and elementwise operations --- no *for* loops!\n",
        "If you want inspiration, read through the code for *Model.compute_activations* and try to understand how the matrix operations correspond to the computations performed by all the units in the network.\n",
        "\n",
        "*Hint: Your implementations should also be similar to* `hid_to_output_weights_grad`,`hid_bias_grad` *in the same function call*\n",
        "\n",
        "*Hint: To prompt a GPT-like model, you may only include functions that are relevent to the implementation in your prompt.*\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "0F4CTBipK9B6"
      },
      "outputs": [],
      "source": [
        "class Model(object):\n",
        "    \"\"\"A class representing the language model itself. This class contains various methods used in training\n",
        "    the model and visualizing the learned representations. It has two fields:\n",
        "\n",
        "        params, a Params instance which contains the model parameters\n",
        "        vocab, a list containing all the words in the dictionary; vocab[0] is the word with index\n",
        "               0, and so on.\"\"\"\n",
        "\n",
        "    def __init__(self, params, vocab):\n",
        "        self.params = params\n",
        "        self.vocab = vocab\n",
        "\n",
        "        self.vocab_size = len(vocab)\n",
        "        self.embedding_dim = self.params.word_embedding_weights.shape[1]\n",
        "        self.embedding_layer_dim = self.params.embed_to_hid_weights.shape[1]\n",
        "        self.context_len = self.embedding_layer_dim // self.embedding_dim\n",
        "        self.num_hid = self.params.embed_to_hid_weights.shape[0]\n",
        "\n",
        "    def copy(self):\n",
        "        return self.__class__(self.params.copy(), self.vocab[:])\n",
        "\n",
        "    @classmethod\n",
        "    def random_init(cls, init_wt, vocab, context_len, embedding_dim, num_hid):\n",
        "        \"\"\"Constructor which randomly initializes the weights to Gaussians with standard deviation init_wt\n",
        "        and initializes the biases to all zeros.\"\"\"\n",
        "        params = Params.random_init(init_wt, len(vocab), context_len, embedding_dim, num_hid)\n",
        "        return Model(params, vocab)\n",
        "\n",
        "    def indicator_matrix(self, targets, mask_zero_index=True):\n",
        "        \"\"\"Construct a matrix where the (v + n*V)th entry of row i is 1 if the n-th target word\n",
        "         for example i is v, and all other entries are 0.\n",
        "\n",
        "         Note: if the n-th target word index is 0, this corresponds to the [MASK] token,\n",
        "               and we set the entry to be 0. \n",
        "        \"\"\"\n",
        "        batch_size, context_len = targets.shape\n",
        "        expanded_targets = np.zeros((batch_size, context_len * len(self.vocab)))\n",
        "        offset = np.repeat((np.arange(context_len) * len(self.vocab))[np.newaxis, :], batch_size, axis=0) # [[0, V, 2V], [0, V, 2V], ...]\n",
        "        targets_offset = targets + offset\n",
        "\n",
        "        for c in range(context_len):\n",
        "          expanded_targets[np.arange(batch_size), targets_offset[:,c]] = 1.\n",
        "          if mask_zero_index:\n",
        "            # Note: Set the targets with index 0, V, 2V to be zero since it corresponds to the [MASK] token\n",
        "            expanded_targets[np.arange(batch_size), offset[:,c]] = 0. \n",
        "        return expanded_targets\n",
        "\n",
        "    def compute_loss_derivative(self, output_activations, expanded_target_batch, target_mask):\n",
        "        \"\"\"Compute the gradient of cross-entropy loss wrt output logits z\n",
        "        \n",
        "            For example:\n",
        "\n",
        "         [y_{0} ....  y_{V-1}] [y_{V}, ..., y_{2*V-1}] [y_{2*V} ... y_{i,3*V-1}] [y_{3*V} ... y_{i,4*V-1}] \n",
        "                  \n",
        "         Where for column v + n*V,\n",
        "\n",
        "            y_{v + n*V} = e^{z_{v + n*V}} / \\sum_{m=0}^{V-1} e^{z_{m + n*V}}, for n=0,...,N-1\n",
        "\n",
        "        This function should return a dC / dz matrix of size [batch_size x (vocab_size * context_len)],\n",
        "        where each row i in dC / dz has columns 0 to V-1 containing the gradient the 1st output \n",
        "        context word from i-th training example, then columns vocab_size to 2*vocab_size - 1 for the 2nd\n",
        "        output context word of the i-th training example, etc.\n",
        "        \n",
        "        C is the loss function summed acrossed all examples as well:\n",
        "\n",
        "            C = -\\sum_{i,j,n} mask_{i,n} (t_{i, j + n*V} log y_{i, j + n*V}), for j=0,...,V, and n=0,...,N\n",
        "\n",
        "        where mask_{i,n} = 1 if the i-th training example has n-th context word as the target, \n",
        "        otherwise mask_{i,n} = 0.\n",
        "        \n",
        "        Args:\n",
        "          output_activations: A [batch_size x (context_len * vocab_size)] matrix, \n",
        "              for the activations of the output layer, i.e. the y_j's.\n",
        "          expanded_target_batch: A [batch_size x (context_len * vocab_size)] matrix, \n",
        "              where expanded_target_batch[i,n*V:(n+1)*V] is the indicator vector for \n",
        "              the n-th context target word position, i.e. the (i, j + n*V) entry is 1 if the \n",
        "              i'th example, the context word at position n is j, and 0 otherwise.\n",
        "          target_mask: A [batch_size x context_len x 1] tensor, where target_mask[i,n] = 1 \n",
        "              if for the i'th example the n-th context word is a target position, otherwise 0\n",
        "        \n",
        "        Outputs:\n",
        "          loss_derivative: A [batch_size x (context_len * vocab_size)] matrix,\n",
        "              where loss_derivative[i,0:vocab_size] contains the gradient\n",
        "              dC / dz_0 for the i-th training example gradient for 1st output \n",
        "              context word, and loss_derivative[i,vocab_size:2*vocab_size] for \n",
        "              the 2nd output context word of the i-th training example, etc.\n",
        "        \"\"\"\n",
        "        # Reshape output_activations and expanded_target_batch and use broadcasting\n",
        "        output_activations_reshape = output_activations.reshape(-1, self.context_len, len(self.vocab))\n",
        "        expanded_target_batch_reshape = expanded_target_batch.reshape(-1, self.context_len, len(self.vocab))\n",
        "        gradient_masked_reshape =  target_mask * (output_activations_reshape - expanded_target_batch_reshape)\n",
        "        gradient_masked = gradient_masked_reshape.reshape(-1, self.context_len * len(self.vocab))\n",
        "        return gradient_masked\n",
        "\n",
        "    def compute_loss(self, output_activations, expanded_target_batch, target_mask):\n",
        "        \"\"\"Compute the total cross entropy loss over a mini-batch.\n",
        "\n",
        "        Args:\n",
        "          output_activations: [batch_size x (context_len * vocab_size)] matrix, \n",
        "                for the activations of the output layer, i.e. the y_j's.\n",
        "          expanded_target_batch: [batch_size (context_len * vocab_size)] matrix, \n",
        "                where expanded_target_batch[i,n*V:(n+1)*V] is the indicator vector for \n",
        "                the n-th context target word position, i.e. the (i, j + n*V) entry is 1 if the \n",
        "                i'th example, the context word at position n is j, and 0 otherwise. matrix obtained\n",
        "          target_mask: A [batch_size x context_len x 1] tensor, where target_mask[i,n,0] = 1 \n",
        "                if for the i'th example the n-th context word is a target position, otherwise 0\n",
        "        \n",
        "        Returns:\n",
        "          loss: a scalar for the  total cross entropy loss over the batch, \n",
        "                defined in Part 3\n",
        "                \n",
        "        In this part, you will learn to implement and train the neural language model from Figure 1. As described in the previous section, during training, we randomly sample one of the $N$ context words to replace with a `[MASK]` token. The goal is for the network to predict the word that was masked, at the corresponding output word position. In practice, this `[MASK]` token is assigned the index 0 in our dictionary. The weights $W^{(2)}$ = `hid_to_output_weights` now has the shape $NV \\times H$, as the output layer has $NV$ neurons, where the first $V$ output units are for predicting the first word, then the next $V$ are for predicting the second word, and so on. \n",
        "        We call this as *concatenating* output units across all word positions, i.e. the $(v + nV)$-th column is for the word $v$ in vocabulary for the $n$-th output word position. \n",
        "        Note here that the softmax is applied in chunks of $V$ as well, to give a valid probability distribution over the $V$ words (For simplicity we also include the `[MASK]` token as one of the possible prediction even though we know the target should not be this token). Only the output word positions that were masked in the input are included in the cross entropy loss calculation:\n",
        "\n",
        "        $$C = -\\sum_{i}^{B}\\sum_{n}^{N}\\sum_{v}^{V} m^{(i)}_{n} (t^{(i)}_{v + nV} \\log y^{(i)}_{v + nV})$$\n",
        "\n",
        "        Where:\n",
        "        *  $y^{(i)}_{v + nV}$ denotes the output probability prediction from the neural network for the $i$-th training example for the word $v$ in the $n$-th output word. Denoting $z$ as the logits output, we define the output probability $y$ as a softmax on $z$ over contiguous chunks of $V$ units (see also Figure 1): \n",
        "\n",
        "        $$y^{(i)}_{v + nV} = \\frac{e^{z^{(i)}_{v+nV}}}{\\sum_{l}^{V} e^{z^{(i)}_{l+nV}}}$$\n",
        "        * $t^{(i)}_{v + nV}  \\in \\{0,1\\}$ is 1 if for the $i$-th training example, the word $v$ is the $n$-th word in context\n",
        "        * $m^{(i)}_{n} \\in \\{0,1\\}$ is a mask that is set to 1 if we are predicting the $n$-th word position for the $i$-th example (because we had masked that word in the input), and 0 otherwise\n",
        "        \"\"\"\n",
        "        ###########################   YOUR CODE HERE  ##############################\n",
        "        # print(output_activations.shape)\n",
        "        # print(expanded_target_batch.shape)\n",
        "        # print(target_mask.shape)\n",
        "        loss = -((expanded_target_batch * np.log(output_activations)).reshape(-1, self.context_len, len(self.vocab)) * target_mask).sum()\n",
        "        ############################################################################\n",
        "        return loss\n",
        "\n",
        "    def compute_activations(self, inputs):\n",
        "        \"\"\"Compute the activations on a batch given the inputs. Returns an Activations instance.\n",
        "        You should try to read and understand this function, since this will give you clues for\n",
        "        how to implement back_propagate.\"\"\"\n",
        "\n",
        "        batch_size = inputs.shape[0]\n",
        "        if inputs.shape[1] != self.context_len:\n",
        "            raise RuntimeError('Dimension of the input vectors should be {}, but is instead {}'.format(\n",
        "                self.context_len, inputs.shape[1]))\n",
        "\n",
        "        # Embedding layer\n",
        "        # Look up the input word indices in the word_embedding_weights matrix\n",
        "        embedding_layer_state = self.params.word_embedding_weights[inputs.reshape([-1]), :].reshape([batch_size, self.embedding_layer_dim])\n",
        "\n",
        "        # Hidden layer\n",
        "        inputs_to_hid = np.dot(embedding_layer_state, self.params.embed_to_hid_weights.T) + \\\n",
        "                        self.params.hid_bias\n",
        "        # Apply logistic activation function\n",
        "        hidden_layer_state = 1. / (1. + np.exp(-inputs_to_hid))\n",
        "\n",
        "        # Output layer\n",
        "        inputs_to_softmax = np.dot(hidden_layer_state, self.params.hid_to_output_weights.T) + \\\n",
        "                            self.params.output_bias\n",
        "\n",
        "        # Subtract maximum.\n",
        "        # Remember that adding or subtracting the same constant from each input to a\n",
        "        # softmax unit does not affect the outputs. So subtract the maximum to\n",
        "        # make all inputs <= 0. This prevents overflows when computing their exponents.\n",
        "        inputs_to_softmax -= inputs_to_softmax.max(1).reshape((-1, 1))\n",
        "\n",
        "        # Take softmax along each V chunks in the output layer\n",
        "        output_layer_state = np.exp(inputs_to_softmax)\n",
        "        output_layer_state_shape = output_layer_state.shape\n",
        "        output_layer_state = output_layer_state.reshape((-1, self.context_len, len(self.vocab)))\n",
        "        output_layer_state /= output_layer_state.sum(axis=-1, keepdims=True) # Softmax along vocab of each target word\n",
        "        output_layer_state = output_layer_state.reshape(output_layer_state_shape) # Flatten back to 2D matrix\n",
        "\n",
        "        return Activations(embedding_layer_state, hidden_layer_state, output_layer_state)\n",
        "\n",
        "    def back_propagate(self, input_batch, activations, loss_derivative):\n",
        "        \"\"\"Compute the gradient of the loss function with respect to the trainable parameters\n",
        "        of the model.\n",
        "        \n",
        "        Part of this function is already completed, but you need to fill in the derivative\n",
        "        computations for hid_to_output_weights_grad, output_bias_grad, embed_to_hid_weights_grad,\n",
        "        and hid_bias_grad. See the documentation for the Params class for a description of what\n",
        "        these matrices represent.\n",
        "\n",
        "        Args: \n",
        "          input_batch: A [batch_size x context_length] matrix containing the \n",
        "              indices of the context words\n",
        "          activations: an Activations object representing the output of \n",
        "              Model.compute_activations\n",
        "          loss_derivative:  A [batch_size x (context_len * vocab_size)] matrix,\n",
        "              where loss_derivative[i,0:vocab_size] contains the gradient\n",
        "              dC / dz_0 for the i-th training example gradient for 1st output \n",
        "              context word, and loss_derivative[i,vocab_size:2*vocab_size] for \n",
        "              the 2nd output context word of the i-th training example, etc.\n",
        "              Obtained from calling compute_loss_derivative()\n",
        "          \n",
        "        Returns:\n",
        "          Params object containing the gradient for word_embedding_weights_grad, \n",
        "              embed_to_hid_weights_grad, hid_to_output_weights_grad,\n",
        "              hid_bias_grad, output_bias_grad  \n",
        "        \"\"\"\n",
        "\n",
        "        # The matrix with values dC / dz_j, where dz_j is the input to the jth hidden unit,\n",
        "        # i.e. h_j = 1 / (1 + e^{-z_j})\n",
        "        hid_deriv = np.dot(loss_derivative, self.params.hid_to_output_weights) \\\n",
        "                    * activations.hidden_layer * (1. - activations.hidden_layer)\n",
        "\n",
        "        \n",
        "        hid_to_output_weights_grad = np.dot(loss_derivative.T, activations.hidden_layer)\n",
        "        \n",
        "        ###########################   YOUR CODE HERE  ##############################\n",
        "        output_bias_grad = loss_derivative.sum(0)\n",
        "        embed_to_hid_weights_grad = np.dot(hid_deriv.T, activations.embedding_layer)\n",
        "        ############################################################################\n",
        "        \n",
        "        hid_bias_grad = hid_deriv.sum(0)\n",
        "\n",
        "        # The matrix of derivatives for the embedding layer\n",
        "        embed_deriv = np.dot(hid_deriv, self.params.embed_to_hid_weights)\n",
        "\n",
        "        # Word Embedding Weights gradient\n",
        "        word_embedding_weights_grad = np.dot(self.indicator_matrix(input_batch.reshape([-1,1]), mask_zero_index=False).T, \n",
        "                                                 embed_deriv.reshape([-1, self.embedding_dim]))\n",
        "\n",
        "        return Params(word_embedding_weights_grad, embed_to_hid_weights_grad, hid_to_output_weights_grad,\n",
        "                      hid_bias_grad, output_bias_grad)\n",
        "\n",
        "    def sample_input_mask(self, batch_size):\n",
        "        \"\"\"Samples a binary mask for the inputs of size batch_size x context_len\n",
        "        For each row, at most one element will be 1.\n",
        "        \"\"\"\n",
        "        mask_idx = np.random.randint(self.context_len, size=(batch_size,))\n",
        "        mask = np.zeros((batch_size, self.context_len), dtype=np.int)# Convert to one hot B x N, B batch size, N context len\n",
        "        mask[np.arange(batch_size), mask_idx] = 1\n",
        "        return mask\n",
        "    \n",
        "    def evaluate(self, inputs, batch_size=100):\n",
        "        \"\"\"Compute the average cross-entropy over a dataset.\n",
        "\n",
        "            inputs: matrix of shape D x N\"\"\"\n",
        "\n",
        "        ndata = inputs.shape[0]\n",
        "\n",
        "        total = 0.\n",
        "        for input_batch in get_batches(inputs, batch_size):\n",
        "            mask = self.sample_input_mask(batch_size)\n",
        "            input_batch_masked = input_batch * (1 - mask)\n",
        "            activations = self.compute_activations(input_batch_masked)\n",
        "            expanded_target_batch = self.indicator_matrix(input_batch)\n",
        "            target_mask = np.expand_dims(mask, axis=2)\n",
        "            cross_entropy = self.compute_loss(activations.output_layer, expanded_target_batch, target_mask)\n",
        "            total += cross_entropy\n",
        "\n",
        "        return total / float(ndata)\n",
        "\n",
        "    def display_nearest_words(self, word, k=10):\n",
        "        \"\"\"List the k words nearest to a given word, along with their distances.\"\"\"\n",
        "\n",
        "        if word not in self.vocab:\n",
        "            print('Word \"{}\" not in vocabulary.'.format(word))\n",
        "            return\n",
        "\n",
        "        # Compute distance to every other word.\n",
        "        idx = self.vocab.index(word)\n",
        "        word_rep = self.params.word_embedding_weights[idx, :]\n",
        "        diff = self.params.word_embedding_weights - word_rep.reshape((1, -1))\n",
        "        distance = np.sqrt(np.sum(diff ** 2, axis=1))\n",
        "\n",
        "        # Sort by distance.\n",
        "        order = np.argsort(distance)\n",
        "        order = order[1:1 + k]  # The nearest word is the query word itself, skip that.\n",
        "        for i in order:\n",
        "            print('{}: {}'.format(self.vocab[i], distance[i]))\n",
        "\n",
        "    def word_distance(self, word1, word2):\n",
        "        \"\"\"Compute the distance between the vector representations of two words.\"\"\"\n",
        "\n",
        "        if word1 not in self.vocab:\n",
        "            raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word1))\n",
        "        if word2 not in self.vocab:\n",
        "            raise RuntimeError('Word \"{}\" not in vocabulary.'.format(word2))\n",
        "\n",
        "        idx1, idx2 = self.vocab.index(word1), self.vocab.index(word2)\n",
        "        word_rep1 = self.params.word_embedding_weights[idx1, :]\n",
        "        word_rep2 = self.params.word_embedding_weights[idx2, :]\n",
        "        diff = word_rep1 - word_rep2\n",
        "        return np.sqrt(np.sum(diff ** 2))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ChatGPT Trace**\n",
        "\n",
        "![](assets/3WcYRKQHUUtM0aGGHzkZJyMG6JqWDQX0EIPZ2OsFoHc.original.fullsize.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbwZCTkboEhz"
      },
      "source": [
        "## 6.3 Print the gradients [0.5pt] \\[Type 4\\]\n",
        "\n",
        "To make your life easier, we have provided the routine `check_gradients`, which checks your gradients using finite differences.\n",
        "        You should make sure this check passes before continuing with the assignment. Once `check_gradients()` passes, call `print_gradients()` and include its output in your write-up. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "B5soRTiRn6W4"
      },
      "outputs": [],
      "source": [
        "def relative_error(a, b):\n",
        "    return np.abs(a - b) / (np.abs(a) + np.abs(b))\n",
        "\n",
        "\n",
        "def check_output_derivatives(model, input_batch, target_batch, mask):\n",
        "    def softmax(z):\n",
        "        z = z.copy()\n",
        "        z -= z.max(-1, keepdims=True)\n",
        "        y = np.exp(z)\n",
        "        y /= y.sum(-1, keepdims=True)\n",
        "        return y\n",
        "\n",
        "    batch_size = input_batch.shape[0]\n",
        "    z = np.random.normal(size=(batch_size, model.context_len, model.vocab_size))\n",
        "    y = softmax(z).reshape((batch_size, model.context_len * model.vocab_size))\n",
        "    z = z.reshape((batch_size, model.context_len * model.vocab_size))\n",
        "\n",
        "    expanded_target_batch = model.indicator_matrix(target_batch)\n",
        "    target_mask = np.expand_dims(mask, axis=2)\n",
        "    loss_derivative = model.compute_loss_derivative(y, expanded_target_batch, target_mask)\n",
        "\n",
        "    if loss_derivative is None:\n",
        "        print('Loss derivative not implemented yet.')\n",
        "        return False\n",
        "\n",
        "    if loss_derivative.shape != (batch_size, model.vocab_size * model.context_len):\n",
        "        print('Loss derivative should be size {} but is actually {}.'.format(\n",
        "            (batch_size, model.vocab_size), loss_derivative.shape))\n",
        "        return False\n",
        "\n",
        "    def obj(z):\n",
        "        z = z.reshape((-1, model.context_len, model.vocab_size))\n",
        "        y = softmax(z).reshape((batch_size, model.context_len * model.vocab_size))\n",
        "        return model.compute_loss(y, expanded_target_batch, target_mask)\n",
        "\n",
        "    for count in range(1000):\n",
        "        i, j = np.random.randint(0, loss_derivative.shape[0]), np.random.randint(0, loss_derivative.shape[1])\n",
        "\n",
        "        z_plus = z.copy()\n",
        "        z_plus[i, j] += EPS\n",
        "        obj_plus = obj(z_plus)\n",
        "\n",
        "        z_minus = z.copy()\n",
        "        z_minus[i, j] -= EPS\n",
        "        obj_minus = obj(z_minus)\n",
        "\n",
        "        empirical = (obj_plus - obj_minus) / (2. * EPS)\n",
        "        rel = relative_error(empirical, loss_derivative[i, j])\n",
        "        if rel > 1e-4:\n",
        "            print('The loss derivative has a relative error of {}, which is too large.'.format(rel))\n",
        "            return False\n",
        "\n",
        "    print('The loss derivative looks OK.')\n",
        "    return True\n",
        "\n",
        "\n",
        "def check_param_gradient(model, param_name, input_batch, target_batch, mask):\n",
        "    activations = model.compute_activations(input_batch)\n",
        "    expanded_target_batch = model.indicator_matrix(target_batch)\n",
        "    target_mask = np.expand_dims(mask, axis=2)\n",
        "    loss_derivative = model.compute_loss_derivative(activations.output_layer, expanded_target_batch, target_mask)\n",
        "    param_gradient = model.back_propagate(input_batch, activations, loss_derivative)\n",
        "\n",
        "    def obj(model):\n",
        "        activations = model.compute_activations(input_batch)\n",
        "        return model.compute_loss(activations.output_layer, expanded_target_batch, target_mask)\n",
        "\n",
        "    dims = getattr(model.params, param_name).shape\n",
        "    is_matrix = (len(dims) == 2)\n",
        "\n",
        "    if getattr(param_gradient, param_name).shape != dims:\n",
        "        print('The gradient for {} should be size {} but is actually {}.'.format(\n",
        "            param_name, dims, getattr(param_gradient, param_name).shape))\n",
        "        return\n",
        "\n",
        "    for count in range(1000):\n",
        "        if is_matrix:\n",
        "            slc = np.random.randint(0, dims[0]), np.random.randint(0, dims[1])\n",
        "        else:\n",
        "            slc = np.random.randint(dims[0])\n",
        "\n",
        "        model_plus = model.copy()\n",
        "        getattr(model_plus.params, param_name)[slc] += EPS\n",
        "        obj_plus = obj(model_plus)\n",
        "\n",
        "        model_minus = model.copy()\n",
        "        getattr(model_minus.params, param_name)[slc] -= EPS\n",
        "        obj_minus = obj(model_minus)\n",
        "\n",
        "        empirical = (obj_plus - obj_minus) / (2. * EPS)\n",
        "        exact = getattr(param_gradient, param_name)[slc]\n",
        "        rel = relative_error(empirical, exact)\n",
        "        if rel > 5e-4:\n",
        "            print('The loss derivative has a relative error of {}, which is too large for param {}.'.format(rel, param_name))\n",
        "            return False\n",
        "\n",
        "    print('The gradient for {} looks OK.'.format(param_name))\n",
        "\n",
        "\n",
        "def load_partially_trained_model():\n",
        "    obj = pickle.load(open(PARTIALLY_TRAINED_MODEL, 'rb'))\n",
        "    params = Params(obj['word_embedding_weights'], obj['embed_to_hid_weights'],\n",
        "                                   obj['hid_to_output_weights'], obj['hid_bias'],\n",
        "                                   obj['output_bias'])\n",
        "    vocab = obj['vocab']\n",
        "    return Model(params, vocab)\n",
        "\n",
        "\n",
        "def check_gradients():\n",
        "    \"\"\"Check the computed gradients using finite differences.\"\"\"\n",
        "    np.random.seed(0)\n",
        "\n",
        "    np.seterr(all='ignore')  # suppress a warning which is harmless\n",
        "\n",
        "    model = load_partially_trained_model()\n",
        "    data_obj = pickle.load(open(data_location, 'rb'))\n",
        "    train_inputs = data_obj['train_inputs']\n",
        "    input_batch = train_inputs[:100, :]\n",
        "    mask = model.sample_input_mask(input_batch.shape[0])\n",
        "    input_batch_masked = input_batch * (1 - mask)\n",
        " \n",
        "    if not check_output_derivatives(model, input_batch_masked, input_batch, mask):\n",
        "        return\n",
        "\n",
        "    for param_name in ['word_embedding_weights', 'embed_to_hid_weights', 'hid_to_output_weights',\n",
        "                       'hid_bias', 'output_bias']:\n",
        "        check_param_gradient(model, param_name, input_batch_masked, input_batch, mask)\n",
        "\n",
        "\n",
        "def print_gradients():\n",
        "    \"\"\"Print out certain derivatives for grading.\"\"\"\n",
        "    np.random.seed(0)\n",
        "\n",
        "    model = load_partially_trained_model()\n",
        "    data_obj = pickle.load(open(data_location, 'rb'))\n",
        "    train_inputs = data_obj['train_inputs']\n",
        "    input_batch = train_inputs[:100, :]\n",
        "\n",
        "    mask = model.sample_input_mask(input_batch.shape[0])\n",
        "    input_batch_masked = input_batch * (1 - mask)\n",
        "    activations = model.compute_activations(input_batch_masked)\n",
        "    expanded_target_batch = model.indicator_matrix(input_batch)\n",
        "    target_mask = np.expand_dims(mask, axis=2)\n",
        "    loss_derivative = model.compute_loss_derivative(activations.output_layer, expanded_target_batch, target_mask)\n",
        "    param_gradient = model.back_propagate(input_batch, activations, loss_derivative)\n",
        "\n",
        "    print('loss_derivative[46, 785]', loss_derivative[46, 785])\n",
        "    print('loss_derivative[46, 766]', loss_derivative[46, 766])\n",
        "    print('loss_derivative[5, 42]', loss_derivative[5, 42])\n",
        "    print('loss_derivative[5, 31]', loss_derivative[5, 31])\n",
        "    print()\n",
        "    print('param_gradient.word_embedding_weights[27, 2]', param_gradient.word_embedding_weights[27, 2])\n",
        "    print('param_gradient.word_embedding_weights[43, 3]', param_gradient.word_embedding_weights[43, 3])\n",
        "    print('param_gradient.word_embedding_weights[22, 4]', param_gradient.word_embedding_weights[22, 4])\n",
        "    print('param_gradient.word_embedding_weights[2, 5]', param_gradient.word_embedding_weights[2, 5])\n",
        "    print()\n",
        "    print('param_gradient.embed_to_hid_weights[10, 2]', param_gradient.embed_to_hid_weights[10, 2])\n",
        "    print('param_gradient.embed_to_hid_weights[15, 3]', param_gradient.embed_to_hid_weights[15, 3])\n",
        "    print('param_gradient.embed_to_hid_weights[30, 9]', param_gradient.embed_to_hid_weights[30, 9])\n",
        "    print('param_gradient.embed_to_hid_weights[35, 21]', param_gradient.embed_to_hid_weights[35, 21])\n",
        "    print()\n",
        "    print('param_gradient.hid_bias[10]', param_gradient.hid_bias[10])\n",
        "    print('param_gradient.hid_bias[20]', param_gradient.hid_bias[20])\n",
        "    print()\n",
        "    print('param_gradient.output_bias[0]', param_gradient.output_bias[0])\n",
        "    print('param_gradient.output_bias[1]', param_gradient.output_bias[1])\n",
        "    print('param_gradient.output_bias[2]', param_gradient.output_bias[2])\n",
        "    print('param_gradient.output_bias[3]', param_gradient.output_bias[3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "6Tlficab3ZfJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3247017/4092977173.py:229: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  mask = np.zeros((batch_size, self.context_len), dtype=np.int)# Convert to one hot B x N, B batch size, N context len\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The loss derivative looks OK.\n",
            "The gradient for word_embedding_weights looks OK.\n",
            "The gradient for embed_to_hid_weights looks OK.\n",
            "The gradient for hid_to_output_weights looks OK.\n",
            "The gradient for hid_bias looks OK.\n",
            "The gradient for output_bias looks OK.\n"
          ]
        }
      ],
      "source": [
        "# Run this to check if your implement gradients matches the finite difference within tolerance\n",
        "# Note: this may take a few minutes to go through all the checks\n",
        "check_gradients()                                   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "1TCLl7v189SI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss_derivative[46, 785] 0.7137561447745507\n",
            "loss_derivative[46, 766] -0.9661570033238931\n",
            "loss_derivative[5, 42] -0.0\n",
            "loss_derivative[5, 31] 0.0\n",
            "\n",
            "param_gradient.word_embedding_weights[27, 2] 0.0\n",
            "param_gradient.word_embedding_weights[43, 3] 0.011596892511489444\n",
            "param_gradient.word_embedding_weights[22, 4] -0.022267062381729714\n",
            "param_gradient.word_embedding_weights[2, 5] 0.0\n",
            "\n",
            "param_gradient.embed_to_hid_weights[10, 2] 0.37932570919301645\n",
            "param_gradient.embed_to_hid_weights[15, 3] 0.016045161321109152\n",
            "param_gradient.embed_to_hid_weights[30, 9] -0.4312854367997418\n",
            "param_gradient.embed_to_hid_weights[35, 21] 0.06679896665436336\n",
            "\n",
            "param_gradient.hid_bias[10] 0.02342880312334519\n",
            "param_gradient.hid_bias[20] -0.024370452378874308\n",
            "\n",
            "param_gradient.output_bias[0] 0.000970106146902794\n",
            "param_gradient.output_bias[1] 0.1686894627476322\n",
            "param_gradient.output_bias[2] 0.0051664774143909235\n",
            "param_gradient.output_bias[3] 0.1509622647181436\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3247017/4092977173.py:229: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  mask = np.zeros((batch_size, self.context_len), dtype=np.int)# Convert to one hot B x N, B batch size, N context len\n"
          ]
        }
      ],
      "source": [
        "# Run this to print out the gradients\n",
        "print_gradients()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FD5Om0ypNPe"
      },
      "source": [
        "To convince us that you have correctly implemented the gradient computations, please include the following with your assignment submission:\n",
        "\n",
        "* [ ] You will submit `a1-code.ipynb` through MarkUs.\n",
        "You do not need to modify any of the code except the parts we asked you to implement.\n",
        "* [ ] In your writeup, include the output of the function `print_gradients`.\n",
        "This prints out part of the gradients for a partially trained network which we have provided, and we will check them against the correct outputs. **Important:** make sure to give the output of `print_gradients`, **not** `check_gradients`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VUBTt0ZQl3s"
      },
      "source": [
        "# Part 7: Bias in Word Embeddings (2pts)\n",
        "\n",
        "Unfortunately, stereotypes and prejudices are often reflected in the outputs of natural language processing algorithms. For example, Google Translate is more likely to translate a non-English sentence to \"_He_ is a doctor\" than \"_She_ is a doctor when the sentence is ambiguous. In this section, you will explore how bias enters natural language processing algorithms by implementing and analyzing a popular method for measuring bias in word embeddings.\n",
        "\n",
        "> Note: In AI and machine learning, **bias** generally refers to prior information, a necessary prerequisite for intelligent action. However, bias can be problematic when it is derived from aspects of human culture known to lead to harmful behaviour, such as stereotypes and prejudices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HlZw3-5Q5XJ"
      },
      "source": [
        "## 7.1 WEAT method for detecting bias [1pt] \\[Type 2\\]\n",
        "\n",
        "   Word embedding models such as GloVe attempt to learn a vector space where semantically similar words are clustered close together. However, they have been shown to learn problematic associations, e.g. by embedding \"man\" more closely to \"doctor\" than \"woman\" (and vice versa for \"nurse\"). To detect such biases in word embeddings, [\"Semantics derived automatically from language corpora contain human-like biases\"](https://www.science.org/doi/10.1126/science.aal4230) introduced the Word Embedding Association Test (WEAT). The WEAT test measures whether two _target_ word sets (e.g., {programmer, engineer, scientist, ...} and {nurse, teacher, librarian, ...}) have the same relative association to two _attribute_ word sets (e.g., man, male, ... and woman, female ...).\n",
        "   \n",
        "> There is an excellent blog on bias in word embeddings and the WEAT test [here](https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html).\n",
        "\n",
        "In the following section, you will run a WEAT test for a given set of target and attribute words. Specifically, you must implement the function `weat_association_score` and then run the remaining cells to compute the p-value and effect size. Before you begin, make sure you understand the formal definition of the WEAT test given in section 4.1 of the handout.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMfmBOJnqNmJ"
      },
      "source": [
        "Run the following cell to download pretrained GloVe embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AI1hYohRQ-lz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded 400000 word embeddings of dimension 50.\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "glove = api.load(\"glove-wiki-gigaword-50\")\n",
        "num_words, num_dims = glove.vectors.shape\n",
        "print(f\"Downloaded {num_words} word embeddings of dimension {num_dims}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0mViYtFnwLR"
      },
      "source": [
        "Before proceeding, you should familiarize yourself with the `similarity` method, which computes the cosine similarity between two words. You will need this method to implement `weat_association_score`. Some examples are given below.\n",
        "\n",
        "> Can you spot the gender bias between occupations in the examples below?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SuRrncLtn5Tl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.49226812\n",
            "0.5718704\n",
            "0.43883625\n",
            "0.715502\n"
          ]
        }
      ],
      "source": [
        "print(glove.similarity(\"man\", \"scientist\"))\n",
        "print(glove.similarity(\"man\", \"nurse\"))\n",
        "print(glove.similarity(\"woman\", \"scientist\"))\n",
        "print(glove.similarity(\"woman\", \"nurse\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlJS8luQoQV5"
      },
      "source": [
        "Below, we define our target words (`occupations`) and attribute words (`A` and `B`). Our target words consist of *occupations*, and our attribute words are *gendered*. We will use the WEAT test to determine if the word embeddings contain gender biases for certain occupations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "H1OEkmNiX-AH"
      },
      "outputs": [],
      "source": [
        "# Target words (occupations)\n",
        "occupations = [\"programmer\", \"engineer\", \"scientist\", \"nurse\", \"teacher\", \"librarian\"]\n",
        "# Two sets of gendered attribute words, A and B\n",
        "A = [\"man\", \"male\", \"he\", \"boyish\"]\n",
        "B = [\"woman\", \"female\", \"she\", \"girlish\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvTau4w1s2es"
      },
      "source": [
        "- [ ] __TODO__: Implement the following function, `weat_association_score` which computes the association of a word _w_ with the attribute:\n",
        "\n",
        "$$s(w, A, B) = \\text{mean}_{a\\in A} \\cos(w, a) - \\text{mean}_{b\\in B} \\cos(w,b)$$\n",
        "\n",
        "*Hint: To prompt a GPT-like model, think about how to provide the information that the `glove` has an method `similarity` which can be called.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FH7KHVgPYyl5"
      },
      "outputs": [],
      "source": [
        "def weat_association_score(w, A, B, glove):\n",
        "    \"\"\"Given a target word w, the set of attribute words A and B,\n",
        "    and the GloVe embeddings, returns the association score s(w, A, B).\n",
        "    \"\"\"\n",
        "    ###########################   YOUR CODE HERE  ##############################\n",
        "    sim_A = [glove.similarity(w, a) for a in A]\n",
        "    mean_sim_A = sum(sim_A) / len(A)\n",
        "    sim_B = [glove.similarity(w, b) for b in B]\n",
        "    mean_sim_B = sum(sim_B) / len(B)\n",
        "    return mean_sim_A - mean_sim_B\n",
        "    ############################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VM9-DCewvsJ"
      },
      "source": [
        "Use the following code to check your implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QlN4D0JRwgpu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.isclose(weat_association_score(\"programmer\", A, B, glove), 0.019615129)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Anbhmfiy_qiU"
      },
      "source": [
        "Now, compute the WEAT association score for each element of `occupations` and the attribute sets A and B. Include the printed out association scores in your pdf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4ld48gnL_ySM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Association score for programmer is: 0.019615095923654735\n",
            "Association score for engineer is: 0.05364736542105675\n",
            "Association score for scientist is: 0.06795817986130714\n",
            "Association score for nurse is: -0.09486919268965721\n",
            "Association score for teacher is: -0.018930237740278244\n",
            "Association score for librarian is: -0.024141353089362383\n"
          ]
        }
      ],
      "source": [
        "# TODO: Print out the weat association score for each occupation\n",
        "###########################   YOUR CODE HERE  ##############################\n",
        "for occupation in occupations:\n",
        "    score = weat_association_score(occupation, A, B, glove)\n",
        "    print(\"Association score for\", occupation, \"is:\", score)\n",
        "############################################################################"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ChatGPT Trace**\n",
        "\n",
        "![](assets/vOuT1kzh9n05nw6Xxfe-_VeY3IV5cOsqLQfCz0aYaM8.original.fullsize.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KzFpg3AFRAp0"
      },
      "source": [
        "## 7.3 Analyzing WEAT [1pt]\n",
        "\n",
        "   While WEAT makes intuitive sense by asserting that closeness in the embedding space indicates greater similarity, more recent work ([Ethayarajh et al. [2019]](https://aclanthology.org/P19-1166.pdf)) has further analyzed the mathematical assertions and found some flaws with this method. Analyzing edge cases is a good way to find logical inconsistencies with any algorithm, and WEAT in particular can behave strangely when A and B contain just one word each. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFvAf7jXWhrS"
      },
      "source": [
        "### 7.3.1 1-word subsets [0.5 pts] \\[Type 4\\]\n",
        "Find 1-word subsets of the original A and B that reverse the sign of the association score for at least some of the occupations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mEuECz_6PEMQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Association score for teacher is: 0.058042705059051514\n",
            "Original score for teacher is: -0.018930237740278244\n",
            "The 1-word sets are: ['man'] ['female']\n",
            "Association score for librarian is: 0.06666575372219086\n",
            "Original score for librarian is: -0.024141353089362383\n",
            "The 1-word sets are: ['man'] ['female']\n",
            "Association score for nurse is: 0.35583968460559845\n",
            "Original score for nurse is: -0.09486919268965721\n",
            "The 1-word sets are: ['man'] ['girlish']\n",
            "Association score for teacher is: 0.4468337446451187\n",
            "Original score for teacher is: -0.018930237740278244\n",
            "The 1-word sets are: ['man'] ['girlish']\n",
            "Association score for librarian is: 0.27873520366847515\n",
            "Original score for librarian is: -0.024141353089362383\n",
            "The 1-word sets are: ['man'] ['girlish']\n",
            "Association score for programmer is: -0.011286124587059021\n",
            "Original score for programmer is: 0.019615095923654735\n",
            "The 1-word sets are: ['male'] ['woman']\n",
            "Association score for engineer is: -0.18472768366336823\n",
            "Original score for engineer is: 0.05364736542105675\n",
            "The 1-word sets are: ['male'] ['woman']\n",
            "Association score for scientist is: -0.16186094284057617\n",
            "Original score for scientist is: 0.06795817986130714\n",
            "The 1-word sets are: ['male'] ['woman']\n",
            "Association score for programmer is: -0.051630899310112\n",
            "Original score for programmer is: 0.019615095923654735\n",
            "The 1-word sets are: ['male'] ['female']\n",
            "Association score for engineer is: -0.08600880205631256\n",
            "Original score for engineer is: 0.05364736542105675\n",
            "The 1-word sets are: ['male'] ['female']\n",
            "Association score for scientist is: -0.06417012214660645\n",
            "Original score for scientist is: 0.06795817986130714\n",
            "The 1-word sets are: ['male'] ['female']\n",
            "Association score for programmer is: -0.02121761441230774\n",
            "Original score for programmer is: 0.019615095923654735\n",
            "The 1-word sets are: ['male'] ['she']\n",
            "Association score for engineer is: -0.21983717381954193\n",
            "Original score for engineer is: 0.05364736542105675\n",
            "The 1-word sets are: ['male'] ['she']\n",
            "Association score for scientist is: -0.13856497406959534\n",
            "Original score for scientist is: 0.06795817986130714\n",
            "The 1-word sets are: ['male'] ['she']\n",
            "Association score for nurse is: 0.3437433987855911\n",
            "Original score for nurse is: -0.09486919268965721\n",
            "The 1-word sets are: ['male'] ['girlish']\n",
            "Association score for teacher is: 0.34907539188861847\n",
            "Original score for teacher is: -0.018930237740278244\n",
            "The 1-word sets are: ['male'] ['girlish']\n",
            "Association score for librarian is: 0.17784558050334454\n",
            "Original score for librarian is: -0.024141353089362383\n",
            "The 1-word sets are: ['male'] ['girlish']\n",
            "Association score for programmer is: -0.00685197114944458\n",
            "Original score for programmer is: 0.019615095923654735\n",
            "The 1-word sets are: ['he'] ['female']\n",
            "Association score for teacher is: 0.08346575498580933\n",
            "Original score for teacher is: -0.018930237740278244\n",
            "The 1-word sets are: ['he'] ['female']\n",
            "Association score for librarian is: 0.15235765278339386\n",
            "Original score for librarian is: -0.024141353089362383\n",
            "The 1-word sets are: ['he'] ['female']\n",
            "Association score for nurse is: 0.2644778937101364\n",
            "Original score for nurse is: -0.09486919268965721\n",
            "The 1-word sets are: ['he'] ['girlish']\n",
            "Association score for teacher is: 0.4722567945718765\n",
            "Original score for teacher is: -0.018930237740278244\n",
            "The 1-word sets are: ['he'] ['girlish']\n",
            "Association score for librarian is: 0.36442710272967815\n",
            "Original score for librarian is: -0.024141353089362383\n",
            "The 1-word sets are: ['he'] ['girlish']\n",
            "Association score for programmer is: -0.1547197625041008\n",
            "Original score for programmer is: 0.019615095923654735\n",
            "The 1-word sets are: ['boyish'] ['woman']\n",
            "Association score for engineer is: -0.41798314452171326\n",
            "Original score for engineer is: 0.05364736542105675\n",
            "The 1-word sets are: ['boyish'] ['woman']\n",
            "Association score for scientist is: -0.3417776823043823\n",
            "Original score for scientist is: 0.06795817986130714\n",
            "The 1-word sets are: ['boyish'] ['woman']\n",
            "Association score for programmer is: -0.19506453722715378\n",
            "Original score for programmer is: 0.019615095923654735\n",
            "The 1-word sets are: ['boyish'] ['female']\n",
            "Association score for engineer is: -0.3192642629146576\n",
            "Original score for engineer is: 0.05364736542105675\n",
            "The 1-word sets are: ['boyish'] ['female']\n",
            "Association score for scientist is: -0.2440868616104126\n",
            "Original score for scientist is: 0.06795817986130714\n",
            "The 1-word sets are: ['boyish'] ['female']\n",
            "Association score for programmer is: -0.16465125232934952\n",
            "Original score for programmer is: 0.019615095923654735\n",
            "The 1-word sets are: ['boyish'] ['she']\n",
            "Association score for engineer is: -0.45309263467788696\n",
            "Original score for engineer is: 0.05364736542105675\n",
            "The 1-word sets are: ['boyish'] ['she']\n",
            "Association score for scientist is: -0.3184817135334015\n",
            "Original score for scientist is: 0.06795817986130714\n",
            "The 1-word sets are: ['boyish'] ['she']\n",
            "Association score for teacher is: 0.03795386850833893\n",
            "Original score for teacher is: -0.018930237740278244\n",
            "The 1-word sets are: ['boyish'] ['girlish']\n",
            "Association score for librarian is: 0.06820434145629406\n",
            "Original score for librarian is: -0.024141353089362383\n",
            "The 1-word sets are: ['boyish'] ['girlish']\n"
          ]
        }
      ],
      "source": [
        "## Original sets provided here for convenience - try commenting out all but one word from each set\n",
        "# Two sets of gendered attribute words, C and D\n",
        "C = [\"man\",\n",
        "     \"male\",\n",
        "     \"he\",\n",
        "     \"boyish\"\n",
        "     ]\n",
        "D = [\"woman\",\n",
        "     \"female\",\n",
        "     \"she\",\n",
        "     \"girlish\"\n",
        "     ]\n",
        "\n",
        "# TODO: Print out the weat association score for each word in occupations, with regards to C and D\n",
        "###########################   YOUR CODE HERE  ##############################\n",
        "for i in range(len(C)):\n",
        "    for j in range(len(D)):\n",
        "        C_subset = [C[i]]\n",
        "        D_subset = [D[j]]\n",
        "        for occupation in occupations:\n",
        "            score = weat_association_score(occupation, C_subset, D_subset, glove)\n",
        "            original_score = weat_association_score(occupation, A, B, glove)\n",
        "            # if the sign of the two scores are different\n",
        "            if np.sign(score) != np.sign(original_score):\n",
        "                print(\"Association score for\", occupation, \"is:\", score)\n",
        "                print(\"Original score for\", occupation, \"is:\", original_score)\n",
        "                print(\"The 1-word sets are:{}, {}\\n\".format(C_subset, D_subset))\n",
        "############################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SStZzhgNVcYA"
      },
      "source": [
        "### 7.3.2  How word frequency affects embedding similarity [0.5 pts] \\[Type 3\\] \\[EC\\]\n",
        "\n",
        "Consider the fact that the squared norm of a word embedding is linear in the log probability of the word in the training corpus. In other words, the more common a word is in the training corpus, the larger the norm of its word embedding. (See handout for more thorough description)\n",
        "    \n",
        "Briefly explain how this fact might contribute to the results from the previous section when using different attribute words. Provide your answers in no more than three sentences.\n",
        "\n",
        "*Hint 2: The paper cited above is a great resource if you are stuck.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**\n",
        "\n",
        "This fact shows that the association score in WEAT can be influenced by the frequency of words in the training corpus. The more common a word is in the corpus, the larger its norm in the word embedding, and the larger its impact on the association score. As a result, the results from the previous section could be biased towards more frequent words, leading to inaccuracies in the assessment of gender bias in the embeddings."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
