{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e4a98a",
   "metadata": {},
   "source": [
    "4.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98543ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss_GloVe(W, W_, b, b_, X, symmetric=False):\n",
    "    V = X.shape[0]\n",
    "    if symmetric:\n",
    "        # Symmetric model\n",
    "        W_ = W\n",
    "        b_ = b\n",
    "    dot = np.dot(W, W_.T)\n",
    "    dot_bias = dot + b + b_.T\n",
    "    dot_bias = np.maximum(dot_bias, 0)\n",
    "    dot_bias = np.log(dot_bias + 1e-9)\n",
    "    X_log = np.log(X + 1e-9)\n",
    "    loss = np.sum((dot_bias - X_log) ** 2)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a5678",
   "metadata": {},
   "source": [
    "4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4fc452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def grad_GloVe(W, W_prime, b, b_prime, X, C, V, d, symmetric=True):\n",
    "    if symmetric:\n",
    "        # W_prime = W and b_prime = b in symmetric case\n",
    "        W_prime = W\n",
    "        b_prime = b\n",
    "    \n",
    "    N = len(X)\n",
    "    grad_W = np.zeros((V, d))\n",
    "    grad_W_prime = np.zeros((V, d))\n",
    "    grad_b = np.zeros(V)\n",
    "    grad_b_prime = np.zeros(V)\n",
    "\n",
    "    for i in range(N):\n",
    "        j, c = X[i][0], X[i][1]\n",
    "        w_i, w_j = W[i, :], W_prime[j, :]\n",
    "        b_i, b_j = b[i], b_prime[j]\n",
    "        \n",
    "        dot = np.dot(w_i, w_j)\n",
    "        dot += b_i + b_j\n",
    "        dot -= np.log(c)\n",
    "        \n",
    "        x = (c / dot) ** 0.5\n",
    "        grad_W[i, :] += x * w_j\n",
    "        grad_W_prime[j, :] += x * w_i\n",
    "        grad_b[i] += x\n",
    "        grad_b_prime[j] += x\n",
    "        \n",
    "        x = -0.5 * x * c / dot\n",
    "        grad_W[i, :] -= x * w_j\n",
    "        grad_W_prime[j, :] -= x * w_i\n",
    "        grad_b[i] -= x\n",
    "        grad_b_prime[j] -= x\n",
    "        \n",
    "    return grad_W, grad_W_prime, grad_b, grad_b_prime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b45228",
   "metadata": {},
   "source": [
    "6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self, input_batch, target_batch, mask_batch):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss for a mini-batch of training examples.\n",
    "\n",
    "    Arguments:\n",
    "    - input_batch: A (B, N, D) numpy array, where B is the batch size, N is the context length,\n",
    "    and D is the input dimension.\n",
    "    - target_batch: A (B, N) numpy array, where B is the batch size, and N is the context length.\n",
    "    The values of target_batch should be indices in the vocabulary.\n",
    "    - mask_batch: A (B, N) numpy array, where B is the batch size, and N is the context length.\n",
    "    The values of mask_batch should be 0 or 1, indicating whether each context word position\n",
    "    is masked or not.\n",
    "\n",
    "    Returns:\n",
    "    - loss: A scalar representing the cross-entropy loss for the mini-batch.\n",
    "    \"\"\"\n",
    "    # Compute the activations for the input batch\n",
    "    activations = self.compute_activations(input_batch)\n",
    "\n",
    "    # Compute the logits and softmax output probabilities\n",
    "    logits = self.hid_to_output_weights @ activations\n",
    "    output_probs = self.softmax(logits)\n",
    "\n",
    "    # Calculate the loss as the sum of cross-entropy losses for the masked positions\n",
    "    loss = 0.0\n",
    "    for i in range(target_batch.shape[0]):\n",
    "        for n in range(target_batch.shape[1]):\n",
    "            if mask_batch[i, n] == 1:\n",
    "                loss -= np.log(output_probs[target_batch[i, n], i * self.vocab_size + n])\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065685c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    ...\n",
    "    \n",
    "    def back_propagate(self, input_batch, target_batch, activations, logits):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the loss with respect to model parameters using backpropagation.\n",
    "\n",
    "        Arguments:\n",
    "        input_batch -- a batch of input data of shape (batch_size, input_dim)\n",
    "        target_batch -- a batch of target data of shape (batch_size, output_dim)\n",
    "        activations -- a list of activations computed by forward pass, starting with input_batch\n",
    "        logits -- a list of logits computed by forward pass, starting with input_batch\n",
    "\n",
    "        Returns:\n",
    "        grad_embed_to_hid_weights -- gradient of the loss with respect to embed_to_hid_weights\n",
    "        grad_hid_to_output_weights -- gradient of the loss with respect to hid_to_output_weights\n",
    "        grad_hid_bias -- gradient of the loss with respect to hid_bias\n",
    "        grad_output_bias -- gradient of the loss with respect to output_bias\n",
    "        \"\"\"\n",
    "        grad_embed_to_hid_weights = np.zeros(self.embed_to_hid_weights.shape)\n",
    "        grad_hid_to_output_weights = np.zeros(self.hid_to_output_weights.shape)\n",
    "        grad_hid_bias = np.zeros(self.hid_bias.shape)\n",
    "        grad_output_bias = np.zeros(self.output_bias.shape)\n",
    "        \n",
    "        delta = self.compute_loss_derivative(target_batch, activations[-1], logits[-1])\n",
    "        grad_hid_to_output_weights = np.matmul(delta.T, activations[-2])\n",
    "        grad_output_bias = np.sum(delta, axis=0)\n",
    "        \n",
    "        delta = np.matmul(delta, self.hid_to_output_weights.T) * (activations[-2] > 0)\n",
    "        grad_hid_bias = np.sum(delta, axis=0)\n",
    "        grad_embed_to_hid_weights = np.matmul(input_batch.T, delta)\n",
    "        \n",
    "        return grad_embed_to_hid_weights, grad_hid_to_output_weights, grad_hid_bias, grad_output_bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea39f80",
   "metadata": {},
   "source": [
    "6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a7647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagate(self, x, t, m):\n",
    "    activations = self.compute_activations(x)\n",
    "    loss_derivative = self.compute_loss_derivative(activations[-1], t, m)\n",
    "    N, V = loss_derivative.shape\n",
    "    _, H = activations[-2].shape\n",
    "    \n",
    "    output_bias_grad = np.sum(loss_derivative, axis=0)\n",
    "    hid_to_output_weights_grad = activations[-2].T @ loss_derivative\n",
    "    hid_error = loss_derivative @ self.hid_to_output_weights.T\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    embed_to_hid_weights_grad = activations[0].T @ hid_error\n",
    "    hid_bias_grad = np.sum(hid_error, axis=0)\n",
    "\n",
    "    return (embed_to_hid_weights_grad, hid_bias_grad, hid_to_output_weights_grad, output_bias_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55643bc3",
   "metadata": {},
   "source": [
    "7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a64d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def weat_association_score(target_words, attribute_words_A, attribute_words_B, word_vectors):\n",
    "    target_word_vectors = np.array([word_vectors[word] for word in target_words])\n",
    "    attribute_vectors_A = np.array([word_vectors[word] for word in attribute_words_A])\n",
    "    attribute_vectors_B = np.array([word_vectors[word] for word in attribute_words_B])\n",
    "\n",
    "    mean_A = np.mean(attribute_vectors_A, axis=0)\n",
    "    mean_B = np.mean(attribute_vectors_B, axis=0)\n",
    "\n",
    "    cosine_similarities_A = np.dot(target_word_vectors, mean_A) / (np.linalg.norm(target_word_vectors, axis=1) * np.linalg.norm(mean_A))\n",
    "    cosine_similarities_B = np.dot(target_word_vectors, mean_B) / (np.linalg.norm(target_word_vectors, axis=1) * np.linalg.norm(mean_B))\n",
    "\n",
    "    return np.mean(cosine_similarities_A) - np.mean(cosine_similarities_B)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
